# ğŸ‘€ ì–‘ì¬ í—ˆë¸Œ ì¸ê³µì§€ëŠ¥ ì˜¤í”ˆì†ŒìŠ¤ ê²½ì§„ëŒ€íšŒ ğŸ‘€
<img width="60%" src="https://user-images.githubusercontent.com/97013710/210364441-89d27d3f-e22e-4156-ad14-b1a73665dd46.jpeg">
ë§‘ì€ ëˆˆì˜ ê´‘ì¸ì´ ë˜ì–´ ì„¸ìƒì„ ë°”ë¼ë³´ì! 


Team : 15ê¸° ì¥ìˆ˜í˜, ì—¼ìœ¤ì„, ìµœë¯¼ê²½, 16ê¸° ë°•ë¯¼ê·œ

<br/><br/>

## Project Descriptions

**Link** : [AI ì–‘ì¬ í—ˆë¸Œ ì¸ê³µì§€ëŠ¥ ì˜¤í”ˆì†ŒìŠ¤ ê²½ì§„ëŒ€íšŒ (DACON)](https://dacon.io/competitions/official/235977/overview/description)

**[ì£¼ì œ]**  
ì´ë¯¸ì§€ ì´ˆí•´ìƒí™”(Image Sper-Resolution)ë¥¼ ìœ„í•œ AI ì•Œê³ ë¦¬ì¦˜ ê°œë°œ  

**[ë°°ê²½]**  
ì˜¤í”ˆ ì†ŒìŠ¤ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¸ê³µì§€ëŠ¥ ì»´í“¨í„° ë¹„ì „ì˜ 'ì´ë¯¸ì§€ ì´ˆí•´ìƒí™”' ë¶„ì•¼ ì—°êµ¬ê°œë°œ  

**[ì„¤ëª…]**  
í’ˆì§ˆì´ ì €í•˜ëœ ì €í•´ìƒë„ ì´¬ì˜ ì´ë¯¸ì§€(512x512)ë¥¼ ê³ í’ˆì§ˆì˜ ê³ í•´ìƒë„ ì´¬ì˜ ì´ë¯¸ì§€(2048x2048)ë¡œ ìƒì„±  

**[í‰ê°€ ì‚°ì‹]**  
PSNR(Peak Signal-to-Noise Ratio) = $10log_{10}(R^2/MSE)$
-	ìƒì„± í˜¹ì€ ì••ì¶•ëœ ì˜ìƒì˜ í™”ì§ˆì— ëŒ€í•œ â€œì†ì‹¤ ì •ë³´â€ë¥¼ í‰ê°€
-	ì†ì‹¤ì´ ì ì„ìˆ˜ë¡(=í™”ì§ˆì´ ì¢‹ì„ ìˆ˜ë¡) ë†’ì€ ê°’  

<br/><br/>

## ğŸ‘£ Score(Public)
RRDB+ : 23.40812(38th)

<br/><br/>

## ğŸŒ Environment
Colab Pro+  
GPU: A100-SXM4-40GB * 1(Main) , Tesla T4*1(Sub)

<br/><br/>

## ğŸ”¥ Competition Strategies

**1. Patches(for data augmentation)**  
- Train patches : original 1640 images â†’ 26240(1640*16) patches (X4 downsampling, non-overlapping)  
- Test patches: original 18 images â†’ 882(18*49) patches (X4 downsampling, overlapping(to remove border artifacts)) 

<br/>

**2. Data Transform**  
Non-destructive transformations (not to add or lose the information)
- Flip  
- Transpose  
- RandomRotate  
- ShiftScaleRotate  

<br/>

**3. Training Methods**
- EarlyStopping  
  > To prevent overfitting  
  > If validation loss does not improve after given patience(=2), training is earlystopped  
- Fine-tuning with pre-trained model  
  > pretrained model : [RRDB_PSNR_x4.pth](https://github.com/xinntao/ESRGAN/tree/master/models)(the PSNR-oriented model withÂ high PSNR performance)  
  > Retraining entire model : Judging that the similarity between DF2K dataset(pretrained model) and our training datset is small  

<br/>

**4. Loss Function**
- L1 loss + L2 loss (2:1)
  > L2 loss : PSNR is based on MSE  
  > L1 loss: For better convergence [https://arxiv.org/pdf/1707.02921v1.pdf](https://arxiv.org/pdf/1707.02921v1.pdf)

<br/>

**5. Learning Scheduler, Optimizer**
- StepLR  
  > step_size = 3, gamma = 0.5  
  > Decays the learning rate of each parameter in half once per 3 epochs 
- Adam  

<br/>

**6. Post Processing**
- Geometric Self-Ensemble [https://arxiv.org/pdf/1707.02921v1.pdf](https://arxiv.org/pdf/1707.02921v1.pdf)
  > <img width="751" alt="KakaoTalk_Photo_2023-01-04-11-58-49" src="https://user-images.githubusercontent.com/55012723/210476203-015eac00-d0e0-4d10-8eb5-a772a9910097.png">


<br/><br/> 

## Main configuration & Hyperparameters
'''
1. Manuel_seed : 42  

2. Model :  
   > num_feat : 64 , Channel number of intermediate features.  
   > growth_channel: 32 , Channels for each growth(dense connection).  
   > num_block: 23 , number of RRDB blocks.  

3. Dataloader :  
   > train_batch_size : 4  
   > test_batch_size: 1  
   > num_workers: 4   

4. Train :  
   > epochs: 7  
   > optim_g: {type: Adam, lr: 1e-4, betas: [0.9, 0.99]}  

'''

<br/><br/>

## Code Descriptions
1. DACON_AISR_TRIAL
- EDSR, SRGAN, SWINIR


2. DACON_AISR_BEST
- RRDB, RRDB+(Self-ensemble)
