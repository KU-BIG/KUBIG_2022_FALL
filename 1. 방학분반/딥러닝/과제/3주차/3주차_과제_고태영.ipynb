{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3주차_과제_고태영.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- dataset을 임의로 선정해서 직접 분석 해보기(제공한 코드를 활용해서 해보기)\n",
        "- activation functions 중 relu사용시 함수 직접 정의\n",
        "- lr, optimizer 등 바꿔보기\n",
        "- hidden layer/neuron 수를 바꾸기\n",
        "- 전처리도 추가\n",
        "- 모든 시도를 올려주세요!\n",
        "- 제일 높은 acc를 보인 시도를 명시해주세요!\n"
      ],
      "metadata": {
        "id": "sgAYo4nrw2F4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX437IL6qbI-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.datasets import load_wine\n",
        "from torch.utils.data import  TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=load_wine()\n",
        "input = data.data\n",
        "output = data.target\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == \"cuda\":\n",
        "  torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "FywYbfsKtjcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input.shape) #178개의 sample input 13"
      ],
      "metadata": {
        "id": "W_ooZVmKCvGa",
        "outputId": "0aa397f4-c078-46a6-a049-4cc61f113427",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(178, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(input, output, test_size = 0.3, random_state = 42, stratify= data.target, shuffle = True)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train).to(device) #데이터를 gpu로\n",
        "y_train = torch.LongTensor(y_train).to(device)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.LongTensor(y_test) ##?\n",
        "\n",
        "# 데이터를 tensor로 바꿔주고 gpu 연산이 가능해지도록 gpu에 옮김\n",
        "# label 값을 왜 long 에 옮겨놓는가? loss function이 다르기 때문 "
      ],
      "metadata": {
        "id": "bLMzf-2ntYeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(y_train[0])\n",
        "\n",
        "#input 13개 (속성이 13개)\n",
        "#y의 class는 3개 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umEdiTZkrVqS",
        "outputId": "df0849e9-3369-46c3-9ab7-643a9bdf65ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.3750e+01, 1.7300e+00, 2.4100e+00, 1.6000e+01, 8.9000e+01, 2.6000e+00,\n",
            "        2.7600e+00, 2.9000e-01, 1.8100e+00, 5.6000e+00, 1.1500e+00, 2.9000e+00,\n",
            "        1.3200e+03], device='cuda:0')\n",
            "tensor(0, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader에 전달하는 것\n",
        "- init : class 에서 객체가 생성되면 바로 실행되는 함수\n",
        "- len : observation 수를 정의하는 함수\n",
        "- getitem : iteration 마다 해당하는 데이터를 돌려주는 함수"
      ],
      "metadata": {
        "id": "combmxzmYFyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#case1: 베이스라인 코드 그대로"
      ],
      "metadata": {
        "id": "WP751MW0sZNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset   # Dataset 은 샘플과 정답(label)을 저장하고, DataLoader 는 Dataset 을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감싼다.\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,transform = None):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]  \n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device) #객체에 []을 하면 자동으로 x,y 저장됨\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "id": "y38TlgXoqV5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) "
      ],
      "metadata": {
        "id": "x8VHwnuFqino"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "          nn.Linear(13,26, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(26,39, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.Linear(39,3, bias=True), \n",
        "          nn.Softmax()\n",
        "          ).to(device)"
      ],
      "metadata": {
        "id": "C6V7a4tyq6Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class로 구현 가능\n",
        "- init : 초기 생성 함수\n",
        "- foward : 순전파(입력값 => 예측값 의 과정)"
      ],
      "metadata": {
        "id": "07uV8RY7Yr_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(13,26, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "        nn.BatchNorm1d(26)   #input 값 맞춰주기\n",
        "    )\n",
        "  # activation function 이용 \n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함 \n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨 \n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨 \n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(26,39, bias=True),nn.Sigmoid()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(39,52, bias=True),\n",
        "          nn.Sigmoid()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(52, 3, bias=True), \n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output\n",
        "\n",
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서    "
      ],
      "metadata": {
        "id": "a0zLstbMqxEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMDUBFg6rUpw",
        "outputId": "f11b20aa-50af-4d02-d4ff-d1fcdc2ffe9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=13, out_features=26, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=26, out_features=39, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=39, out_features=52, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=52, out_features=3, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZt5CetrYFb",
        "outputId": "3941b24a-c3de-462b-d337-1f5bc2975073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (layer1): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=26, bias=True)\n",
            "    (1): Sigmoid()\n",
            "    (2): BatchNorm1d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Linear(in_features=26, out_features=39, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Linear(in_features=39, out_features=52, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Linear(in_features=52, out_features=10, bias=True)\n",
            "    (1): Softmax(dim=None)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "AYFp-eTErh7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90QxHvlIrjS7",
        "outputId": "35c71b52-d6dd-4e74-bdf5-a1f89fa7a800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.153936743736267\n",
            "10 1.0552979707717896\n",
            "20 0.9223798513412476\n",
            "30 0.8678139448165894\n",
            "40 0.8625806570053101\n",
            "50 0.8327105045318604\n",
            "60 0.8046091794967651\n",
            "70 0.7563766241073608\n",
            "80 0.7105939984321594\n",
            "90 0.653317928314209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "81ASYrW7roFM",
        "outputId": "39f38c9b-88a9-4e3b-fc63-aa686d80855b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4kJzpLErqhZ",
        "outputId": "1eff021f-1f68-42e2-d806-a24c8b96015f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'model의 output은 : {y_pred[0]}')\n",
        "print(f'argmax를 한 후의 output은 {predicted[0]}')\n",
        "print(f'accuracy는 {accuracy}')   #초기 accuracy 약 0.91"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIKhs3Nr6Ay",
        "outputId": "c68fd358-77fe-4d7d-8c65-b308fd520fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model의 output은 : [9.9825186e-01 5.3679406e-07 1.7474985e-03]\n",
            "argmax를 한 후의 output은 0\n",
            "accuracy는 0.9074074074074074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# case 2: 전처리: normalize 추가"
      ],
      "metadata": {
        "id": "esFgd8Des5_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.mean(),x_train.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g0f7dvcxHOs",
        "outputId": "e36ea980-976a-450a-d1ce-3ec04b376968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.8869, device='cuda:0') tensor(6.0144, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset  \n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,transform = None):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]  \n",
        "    self.transform = transform \n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device) #객체에 []을 하면 자동으로 x,y 저장됨\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "torchvision_transform = transforms.Normalize(4.8869, 6.0144, inplace=False)  #추가한 부분\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "dataset = CustomDataset(transform=torchvision_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) "
      ],
      "metadata": {
        "id": "5pArVqxXtHjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(13,26, bias=True), # input_layer = 30, hidden_layer1 = 398 \n",
        "          nn.Sigmoid(),\n",
        "        nn.BatchNorm1d(26)   #input 값 맞춰주기\n",
        "    )\n",
        "  # activation function 이용 \n",
        "  #   nn.ReLU()\n",
        "  #   nn.tanH()\n",
        "  #   https://pytorch.org/docs/stable/nn.html 그 외에도 여기서 확인 가능함 \n",
        "  #   파라미터가 필요하지 않다는 것이 특징\n",
        "\n",
        "  # batch normazliation 1d, 파라미터 값으로 vector의 길이를 전해줌\n",
        "  # 추후에 이미지를 다루게 된다면, 그때는 batch normalization 2d를 이용하게 됨 \n",
        "  # 그때는 파라미터 값으로 채널, 가로, 세로 길이를 전달해주게 됨 \n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(26,39, bias=True),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(39,52, bias=True), \n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Linear(52, 3, bias=True), \n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    return output\n",
        "\n",
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)  \n",
        "\n",
        "model = Model().to(device)\n",
        "model.apply(init_weights)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0H5OBx-t86r",
        "outputId": "bff4b11c-7f13-4529-91df-56d1d316326b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layer1): Sequential(\n",
              "    (0): Linear(in_features=13, out_features=26, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): BatchNorm1d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Linear(in_features=26, out_features=39, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Linear(in_features=39, out_features=52, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Linear(in_features=52, out_features=3, bias=True)\n",
              "    (1): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#module-torch.optim 페이지 참조\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "# sgd 등등"
      ],
      "metadata": {
        "id": "ik7KYHpCuA1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl3jdKGouNar",
        "outputId": "0a87e86e-c495-4785-e4c5-12316f5e2ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.1176129579544067\n",
            "10 0.9931424856185913\n",
            "20 0.8746321201324463\n",
            "30 0.8510243892669678\n",
            "40 0.8474462628364563\n",
            "50 0.8404250144958496\n",
            "60 0.8381294012069702\n",
            "70 0.8249585032463074\n",
            "80 0.7662920951843262\n",
            "90 0.764864444732666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))\n",
        "\n",
        "print(f'accuracy는 {accuracy}')  #0.91로 성능 동일(normarlize가 이미 포함된 데이터임)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIlPwN3LuN09",
        "outputId": "45666410-ae86-4e06-9721-d26c5a749d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy는 0.5555555555555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#case 3: relu 함수+ layer 수 1개로 줄이기"
      ],
      "metadata": {
        "id": "NOFGMOEOtaxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "\treturn np.maximum(0, x)"
      ],
      "metadata": {
        "id": "3fZcEjYatdEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset  \n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,transform = None):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]  \n",
        "    self.transform = transform \n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device) #객체에 []을 하면 자동으로 x,y 저장됨\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "torchvision_transform = transforms.Normalize(4.8869, 6.0144, inplace=False)  #추가한 부분\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "dataset = CustomDataset(transform=torchvision_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) "
      ],
      "metadata": {
        "id": "XgXulYTL1vjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(13,26, bias=True),\n",
        "          nn.ReLU(),\n",
        "        nn.BatchNorm1d(26)   #input 값 맞춰주기\n",
        "    ) \n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(26,3, bias=True), \n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    return output\n",
        "\n",
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)  \n",
        "\n",
        "model = Model().to(device)\n",
        "model.apply(init_weights)    \n",
        "\n",
        "\n",
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n",
        "\n",
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())\n",
        "\n",
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))\n",
        "\n",
        "print(f'accuracy는 {accuracy}')  #0.72로 낮아짐 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXW3gRD0z9i0",
        "outputId": "1c43c18e-1e89-4655-8c64-0119f3851244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.246024489402771\n",
            "10 0.7162477970123291\n",
            "20 0.6224349737167358\n",
            "30 0.5459978580474854\n",
            "40 0.48217323422431946\n",
            "50 0.43875351548194885\n",
            "60 0.39589571952819824\n",
            "70 0.3665273189544678\n",
            "80 0.34330540895462036\n",
            "90 0.3233625590801239\n",
            "accuracy는 0.7222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# case 4: relu 함수+ layer 수 그대로+노드 바꿔보기+lr->0.03\n",
        "\n"
      ],
      "metadata": {
        "id": "e-VmGr4d1T_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset  \n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,transform = None):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]  \n",
        "    self.transform = transform \n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device) #객체에 []을 하면 자동으로 x,y 저장됨\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "torchvision_transform = transforms.Normalize(4.8869, 6.0144, inplace=False)  #추가한 부분\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "dataset = CustomDataset(transform=torchvision_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) "
      ],
      "metadata": {
        "id": "eMyjFdGw1wrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model1(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model1, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(13,10, bias=True), \n",
        "          nn.ReLU(),\n",
        "        nn.BatchNorm1d(10)   #input 값 맞춰주기\n",
        "    )\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(10,7, bias=True),\n",
        "          nn.ReLU()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(7,6, bias=True),\n",
        "          nn.ReLU())\n",
        "    \n",
        "    self.layer4 = nn.Sequential(\n",
        "          nn.Linear(6,3, bias=True),\n",
        "          nn.ReLU()\n",
        "    )    \n",
        "    \n",
        "  def forward(self,x):\n",
        "      output = self.layer1(x)\n",
        "      output = self.layer2(output)\n",
        "      output = self.layer3(output)\n",
        "      output = self.layer4(output)\n",
        "      return output\n",
        "\n",
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서  \n",
        "\n",
        "model = Model1().to(device)\n",
        "model.apply(init_weights)    \n",
        "\n",
        "\n",
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.03)\n",
        "\n",
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())\n",
        "\n",
        "with torch.no_grad():\n",
        "  model1 = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))\n",
        "\n",
        "print(f'accuracy는 {accuracy}')  #0.94"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vQGeRdn1yZe",
        "outputId": "2bc9cc77-403b-4322-c9bd-5459d8ad45be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.0402253866195679\n",
            "10 0.7123870253562927\n",
            "20 0.5942121148109436\n",
            "30 0.5908019542694092\n",
            "40 0.4673052430152893\n",
            "50 0.45830318331718445\n",
            "60 0.4081529378890991\n",
            "70 0.2634502053260803\n",
            "80 0.17430885136127472\n",
            "90 0.07419569790363312\n",
            "accuracy는 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# case 5: relu 함수+ layer 수 그대로+노드 바꿔보기+lr:0.05+dropout"
      ],
      "metadata": {
        "id": "eyNxvRF_2lRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset  \n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,transform = None):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]  \n",
        "    self.transform = transform \n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device) #객체에 []을 하면 자동으로 x,y 저장됨\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "torchvision_transform = transforms.Normalize(4.8869, 6.0144, inplace=False)  #추가한 부분\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "dataset = CustomDataset(transform=torchvision_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) "
      ],
      "metadata": {
        "id": "AEST0B0d2mSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model2(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model2, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(13,10, bias=True), \n",
        "          nn.ReLU(),\n",
        "        nn.BatchNorm1d(10)   #input 값 맞춰주기\n",
        "    )\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(10,7, bias=True),\n",
        "          nn.ReLU()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(7,6, bias=True),\n",
        "          nn.ReLU())\n",
        "    \n",
        "    self.layer4 = nn.Sequential(\n",
        "          nn.Linear(6,3, bias=True),\n",
        "          nn.ReLU()\n",
        "    ) \n",
        "\n",
        "    self.dropout = nn.Dropout(0.25)      \n",
        "    \n",
        "  def forward(self,x):\n",
        "      output = self.layer1(x)\n",
        "      output = self.layer2(output)\n",
        "      output = self.layer3(output)\n",
        "      output = self.dropout(output)      \n",
        "      output = self.layer4(output)\n",
        "      return output\n",
        "\n",
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서  \n",
        "\n",
        "model = Model().to(device)\n",
        "model.apply(init_weights)    \n",
        "\n",
        "\n",
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.05)\n",
        "\n",
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())\n",
        "\n",
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))\n",
        "\n",
        "print(f'accuracy는 {accuracy}')  #0.67로 낮아짐"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTSdFV1A2sp9",
        "outputId": "db477f28-e37c-4fbf-abe8-7ccfc4b05e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.2648327350616455\n",
            "10 0.8121306896209717\n",
            "20 0.7291269898414612\n",
            "30 0.7110924124717712\n",
            "40 0.7343065142631531\n",
            "50 0.6494840383529663\n",
            "60 0.6406269669532776\n",
            "70 0.5395769476890564\n",
            "80 0.5073202848434448\n",
            "90 0.5579782724380493\n",
            "accuracy는 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# case 6: relu 함수+ layer 수 그대로+노드 바꿔보기+lr:0.05+SGD"
      ],
      "metadata": {
        "id": "UjdjnS-uGkTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset  \n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,transform = None):\n",
        "    self.x_data = x_train\n",
        "    self.y_data = [[y] for y in y_train]  \n",
        "    self.transform = transform \n",
        "#  데이터셋의 전처리를 해주는 부분\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx]).to(device) #객체에 []을 하면 자동으로 x,y 저장됨\n",
        "    y = torch.LongTensor(self.y_data[idx]).to(device)\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "torchvision_transform = transforms.Normalize(4.8869, 6.0144, inplace=False)  #추가한 부분\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "dataset = CustomDataset(transform=torchvision_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size) "
      ],
      "metadata": {
        "id": "JbF5qaBWHC_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "          nn.Linear(13,10, bias=True), \n",
        "          nn.ReLU(),\n",
        "        nn.BatchNorm1d(10)   #input 값 맞춰주기\n",
        "    )\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "          nn.Linear(10,7, bias=True),\n",
        "          nn.ReLU()\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "          nn.Linear(7,6, bias=True),\n",
        "          nn.ReLU())\n",
        "    \n",
        "    self.layer4 = nn.Sequential(\n",
        "          nn.Linear(6,3, bias=True),\n",
        "          nn.ReLU()\n",
        "    ) \n",
        "   \n",
        "    \n",
        "  def forward(self,x):\n",
        "      output = self.layer1(x)\n",
        "      output = self.layer2(output)\n",
        "      output = self.layer3(output)    \n",
        "      output = self.layer4(output)\n",
        "      return output\n",
        "\n",
        "def init_weights(layer):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(layer.weight)\n",
        "        layer.bias.data.fill_(0.01)\n",
        "\n",
        "        #xavier사용\n",
        "        # Layer의 weight를 어떤 분포를 가지도록 초기화시켜줌+global minimum찾기 위해서  \n",
        "\n",
        "model = Model().to(device)\n",
        "model.apply(init_weights)    \n",
        "\n",
        "\n",
        "loss_fn  = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr= 0.05)\n",
        "\n",
        "losses = []\n",
        "for epoch in range(100):\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = model(x_train)\n",
        "\n",
        "  # 비용 함수\n",
        "  cost = loss_fn(hypothesis, y_train)\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(cost.item())\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(epoch, cost.item())\n",
        "\n",
        "with torch.no_grad():\n",
        "  model = model.to('cpu')\n",
        "  y_pred = model(x_test)\n",
        "  y_pred = y_pred.detach().numpy()\n",
        "  predicted = np.argmax(y_pred, axis =1)\n",
        "  accuracy = (accuracy_score(predicted, y_test))\n",
        "\n",
        "print(f'accuracy는 {accuracy}')  #0.33으로 낮아짐"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1opTlnjbHGoZ",
        "outputId": "35ddaf50-de3c-4b30-bc90-05929c4c3168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.1428662538528442\n",
            "10 0.9605160355567932\n",
            "20 0.8848334550857544\n",
            "30 0.8578853011131287\n",
            "40 0.8469720482826233\n",
            "50 0.8414644598960876\n",
            "60 0.8373143672943115\n",
            "70 0.8361294269561768\n",
            "80 0.8343256711959839\n",
            "90 0.8335631489753723\n",
            "accuracy는 0.3333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##이외에도 여러번 시도 결과\n",
        "\n",
        "##case4:relu 함수+ layer 수 그대로(4개)+노드 바꿔보기(13,10,7,6,3)+lr->0.03 이 약 94%로 가장 높았음"
      ],
      "metadata": {
        "id": "xKRicxngJdxs"
      }
    }
  ]
}