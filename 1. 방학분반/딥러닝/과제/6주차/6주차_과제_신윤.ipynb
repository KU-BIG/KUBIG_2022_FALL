{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6주차_과제.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nmQ5F7UAeKB_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task1\n",
        "\n",
        "빈 칸을 채워주세요!\n",
        "\n",
        "단계별 output이 github 파일에는 남아있으니 그 output과 동일한 형태인지 확인하면서 진행해주시면 됩니다~"
      ],
      "metadata": {
        "id": "Sgxd6SxmeVcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 생성할 문장 데이터\n",
        "\n",
        "sentence = (\"Brick walls are there for a reason and you must not think \"\n",
        "            \"that the brick walls aren't there to keep us out, but rather \"\n",
        "            \"in this way that the brick walls are there to show us how badly we want things.\")"
      ],
      "metadata": {
        "id": "NDvUeC8BoUb6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. 문자 집합 만들기\n",
        "world_set = list(set(sentence))\n",
        "\n",
        "## 문제(1): 각 문자에 정수 인코딩 (공백도 하나의 원소로 포함)\n",
        "vocab = {c:i for i, c in enumerate(world_set)}"
      ],
      "metadata": {
        "id": "b9lkrKyZf8ie"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_0we5Y-gYDq",
        "outputId": "39950f26-84aa-41bc-f91c-4f83b230c6b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'b': 0, 'a': 1, 'u': 2, 'e': 3, 'i': 4, '.': 5, 'p': 6, 'f': 7, 'm': 8, 'B': 9, 'h': 10, ',': 11, 't': 12, 'r': 13, 'g': 14, 'y': 15, 'k': 16, 'd': 17, ' ': 18, 'w': 19, 'l': 20, 'o': 21, 's': 22, \"'\": 23, 'c': 24, 'n': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. 문자 집합 크기 확인\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('문자 집합 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpKupU6lgpfT",
        "outputId": "88c14e58-f438-46f1-8642-a1eb6d269cba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 크기 : 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
        "\n",
        "hidden_size = vocab_size # 같아야 하는 것 확인!\n",
        "sequence_length = 10  # 너무 길거나 너무 짧게 잡으면 안됩니다!\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "wFDZJHSMg9In"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. seqence 길이 단위 자르기\n",
        "\n",
        "# 데이터 구성을 위한 리스트\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "## 문제(2): 반복문 내에서의 인덱싱을 사용하여 sequence_length 값 단위로 샘플을 잘라 데이터 만들기, y_str은 x_str은 한 칸씩 쉬프트된 sequnce\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "  x_str = sentence[i:i + sequence_length]\n",
        "  y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "  print(i, x_str, \"->\", y_str)\n",
        "\n",
        "  # x_str과 y_str이 문자집합에 해당하는 인덱스를 각각 x_data, y_data에 append\n",
        "  x_data.append([vocab[c] for c in x_str])\n",
        "  y_data.append([vocab[d] for d in y_str])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbDcmJmghN7V",
        "outputId": "a8e5e079-c90c-41d0-d091-09ea8653349b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Brick wall -> rick walls\n",
            "1 rick walls -> ick walls \n",
            "2 ick walls  -> ck walls a\n",
            "3 ck walls a -> k walls ar\n",
            "4 k walls ar ->  walls are\n",
            "5  walls are -> walls are \n",
            "6 walls are  -> alls are t\n",
            "7 alls are t -> lls are th\n",
            "8 lls are th -> ls are the\n",
            "9 ls are the -> s are ther\n",
            "10 s are ther ->  are there\n",
            "11  are there -> are there \n",
            "12 are there  -> re there f\n",
            "13 re there f -> e there fo\n",
            "14 e there fo ->  there for\n",
            "15  there for -> there for \n",
            "16 there for  -> here for a\n",
            "17 here for a -> ere for a \n",
            "18 ere for a  -> re for a r\n",
            "19 re for a r -> e for a re\n",
            "20 e for a re ->  for a rea\n",
            "21  for a rea -> for a reas\n",
            "22 for a reas -> or a reaso\n",
            "23 or a reaso -> r a reason\n",
            "24 r a reason ->  a reason \n",
            "25  a reason  -> a reason a\n",
            "26 a reason a ->  reason an\n",
            "27  reason an -> reason and\n",
            "28 reason and -> eason and \n",
            "29 eason and  -> ason and y\n",
            "30 ason and y -> son and yo\n",
            "31 son and yo -> on and you\n",
            "32 on and you -> n and you \n",
            "33 n and you  ->  and you m\n",
            "34  and you m -> and you mu\n",
            "35 and you mu -> nd you mus\n",
            "36 nd you mus -> d you must\n",
            "37 d you must ->  you must \n",
            "38  you must  -> you must n\n",
            "39 you must n -> ou must no\n",
            "40 ou must no -> u must not\n",
            "41 u must not ->  must not \n",
            "42  must not  -> must not t\n",
            "43 must not t -> ust not th\n",
            "44 ust not th -> st not thi\n",
            "45 st not thi -> t not thin\n",
            "46 t not thin ->  not think\n",
            "47  not think -> not think \n",
            "48 not think  -> ot think t\n",
            "49 ot think t -> t think th\n",
            "50 t think th ->  think tha\n",
            "51  think tha -> think that\n",
            "52 think that -> hink that \n",
            "53 hink that  -> ink that t\n",
            "54 ink that t -> nk that th\n",
            "55 nk that th -> k that the\n",
            "56 k that the ->  that the \n",
            "57  that the  -> that the b\n",
            "58 that the b -> hat the br\n",
            "59 hat the br -> at the bri\n",
            "60 at the bri -> t the bric\n",
            "61 t the bric ->  the brick\n",
            "62  the brick -> the brick \n",
            "63 the brick  -> he brick w\n",
            "64 he brick w -> e brick wa\n",
            "65 e brick wa ->  brick wal\n",
            "66  brick wal -> brick wall\n",
            "67 brick wall -> rick walls\n",
            "68 rick walls -> ick walls \n",
            "69 ick walls  -> ck walls a\n",
            "70 ck walls a -> k walls ar\n",
            "71 k walls ar ->  walls are\n",
            "72  walls are -> walls aren\n",
            "73 walls aren -> alls aren'\n",
            "74 alls aren' -> lls aren't\n",
            "75 lls aren't -> ls aren't \n",
            "76 ls aren't  -> s aren't t\n",
            "77 s aren't t ->  aren't th\n",
            "78  aren't th -> aren't the\n",
            "79 aren't the -> ren't ther\n",
            "80 ren't ther -> en't there\n",
            "81 en't there -> n't there \n",
            "82 n't there  -> 't there t\n",
            "83 't there t -> t there to\n",
            "84 t there to ->  there to \n",
            "85  there to  -> there to k\n",
            "86 there to k -> here to ke\n",
            "87 here to ke -> ere to kee\n",
            "88 ere to kee -> re to keep\n",
            "89 re to keep -> e to keep \n",
            "90 e to keep  ->  to keep u\n",
            "91  to keep u -> to keep us\n",
            "92 to keep us -> o keep us \n",
            "93 o keep us  ->  keep us o\n",
            "94  keep us o -> keep us ou\n",
            "95 keep us ou -> eep us out\n",
            "96 eep us out -> ep us out,\n",
            "97 ep us out, -> p us out, \n",
            "98 p us out,  ->  us out, b\n",
            "99  us out, b -> us out, bu\n",
            "100 us out, bu -> s out, but\n",
            "101 s out, but ->  out, but \n",
            "102  out, but  -> out, but r\n",
            "103 out, but r -> ut, but ra\n",
            "104 ut, but ra -> t, but rat\n",
            "105 t, but rat -> , but rath\n",
            "106 , but rath ->  but rathe\n",
            "107  but rathe -> but rather\n",
            "108 but rather -> ut rather \n",
            "109 ut rather  -> t rather i\n",
            "110 t rather i ->  rather in\n",
            "111  rather in -> rather in \n",
            "112 rather in  -> ather in t\n",
            "113 ather in t -> ther in th\n",
            "114 ther in th -> her in thi\n",
            "115 her in thi -> er in this\n",
            "116 er in this -> r in this \n",
            "117 r in this  ->  in this w\n",
            "118  in this w -> in this wa\n",
            "119 in this wa -> n this way\n",
            "120 n this way ->  this way \n",
            "121  this way  -> this way t\n",
            "122 this way t -> his way th\n",
            "123 his way th -> is way tha\n",
            "124 is way tha -> s way that\n",
            "125 s way that ->  way that \n",
            "126  way that  -> way that t\n",
            "127 way that t -> ay that th\n",
            "128 ay that th -> y that the\n",
            "129 y that the ->  that the \n",
            "130  that the  -> that the b\n",
            "131 that the b -> hat the br\n",
            "132 hat the br -> at the bri\n",
            "133 at the bri -> t the bric\n",
            "134 t the bric ->  the brick\n",
            "135  the brick -> the brick \n",
            "136 the brick  -> he brick w\n",
            "137 he brick w -> e brick wa\n",
            "138 e brick wa ->  brick wal\n",
            "139  brick wal -> brick wall\n",
            "140 brick wall -> rick walls\n",
            "141 rick walls -> ick walls \n",
            "142 ick walls  -> ck walls a\n",
            "143 ck walls a -> k walls ar\n",
            "144 k walls ar ->  walls are\n",
            "145  walls are -> walls are \n",
            "146 walls are  -> alls are t\n",
            "147 alls are t -> lls are th\n",
            "148 lls are th -> ls are the\n",
            "149 ls are the -> s are ther\n",
            "150 s are ther ->  are there\n",
            "151  are there -> are there \n",
            "152 are there  -> re there t\n",
            "153 re there t -> e there to\n",
            "154 e there to ->  there to \n",
            "155  there to  -> there to s\n",
            "156 there to s -> here to sh\n",
            "157 here to sh -> ere to sho\n",
            "158 ere to sho -> re to show\n",
            "159 re to show -> e to show \n",
            "160 e to show  ->  to show u\n",
            "161  to show u -> to show us\n",
            "162 to show us -> o show us \n",
            "163 o show us  ->  show us h\n",
            "164  show us h -> show us ho\n",
            "165 show us ho -> how us how\n",
            "166 how us how -> ow us how \n",
            "167 ow us how  -> w us how b\n",
            "168 w us how b ->  us how ba\n",
            "169  us how ba -> us how bad\n",
            "170 us how bad -> s how badl\n",
            "171 s how badl ->  how badly\n",
            "172  how badly -> how badly \n",
            "173 how badly  -> ow badly w\n",
            "174 ow badly w -> w badly we\n",
            "175 w badly we ->  badly we \n",
            "176  badly we  -> badly we w\n",
            "177 badly we w -> adly we wa\n",
            "178 adly we wa -> dly we wan\n",
            "179 dly we wan -> ly we want\n",
            "180 ly we want -> y we want \n",
            "181 y we want  ->  we want t\n",
            "182  we want t -> we want th\n",
            "183 we want th -> e want thi\n",
            "184 e want thi ->  want thin\n",
            "185  want thin -> want thing\n",
            "186 want thing -> ant things\n",
            "187 ant things -> nt things.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력해서 한 칸씩 쉬프트된 것 확인하기!\n",
        "\n",
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVFlILiOixdc",
        "outputId": "6c3b3f98-9eff-4ba3-9d3a-0d8332f925bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9, 13, 4, 24, 16, 18, 19, 1, 20, 20]\n",
            "[13, 4, 24, 16, 18, 19, 1, 20, 20, 22]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##6. 입력 시퀀스에 대해 원핫인코딩 수행\n",
        "\n",
        "## 문제(4) : x_data를 원핫인코딩 > numpy의 eye를 쓸 수 있지 않을까?\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "\n",
        "##7. 입력 데이터, 레이블데이터 텐서로 변환\n",
        "\n",
        "## 문제(5) : x_one_hot과 y_data 텐서로 변환 : 둘 다 같은 형식의 텐서로 변환하면 될까?? (FloatTensor, LongTesor 중 맞는 것은?)\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data) "
      ],
      "metadata": {
        "id": "5lPes1dvjlNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567efac0-5434-497b-b3e3-a9da5342de13"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##8. 크기 확인\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMZzZlaymMk8",
        "outputId": "a8f9159b-a08f-4863-e827-73a3e7c4670c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([188, 10, 26])\n",
            "레이블의 크기 : torch.Size([188, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##9.원핫인코딩 결과 샘플 확인하기\n",
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knx1DE_AmSFB",
        "outputId": "27bd577d-233b-4e15-bf19-1de2eed1c132"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##10. 레이블 데이터 샘플 확인하기\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pWDiH1SmYT_",
        "outputId": "c3992915-8334-4b66-ec10-c93a6be4c6b6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([13,  4, 24, 16, 18, 19,  1, 20, 20, 22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##11. RNN 모델 구현\n",
        "\n",
        "##문제(6) : 기본 pytorch 인자 넣기 연습 + forward 채우기\n",
        "### 조건 : rnn layer 2개 쌓기 + 마지막은 fc layer\n",
        "### batch_fisrt 설정 필요할까? (유튜브 강의 참고)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "-Ww22xu8mfUc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(vocab_size, hidden_size, 2)"
      ],
      "metadata": {
        "id": "No2GRvTpnLBl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##12. loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "##13. optimizer\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "9-zuJLeUnQLB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##14. 출력 크기 점검\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-RxRaiHnh9U",
        "outputId": "eac99bca-9b25-4b23-bb43-561047b6b8da"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([188, 10, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##15. Training 시작\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    ##문제(7) : outputs, Y 형태 그대로 넣으면 안되죠. view 함수를 이용해 loss값을 계산해봅시다.\n",
        "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #16. 예측결과 확인\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오기\n",
        "            predict_str += ''.join([world_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += world_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxxrxCd2nwoo",
        "outputId": "e12784ab-b696-49d0-ba54-f248cc73adf6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaaakaaaakkamaaaaakaaaaaaaaaakamamaabaaaaaaaamaabaaaaaabkaaamaaaakaabaakaaaakkamaamaaaaakaaaaaakkmmaakaaaakaabaaaaaakaaabaaaakaaaaaaamaaaakaabaakaaaakkamaaaaakaaaaaamamkaikaaakaabaakakkaaabaaaaakam\n",
            "t    aaaa  a    aa   a   a a      a   a  aa   aa  aaaa   aaa aaaa  a    aaaa  a     araa   aa a    a  a  a aa aa  aa  a  aaa  a a aaa aaaa  a    aaaa  a    aa   aa a a aa  aa aaa  a aa   a a aa    \n",
            "t                                                                                                                                                                                                    \n",
            "t                                                                                                                                                                                                    \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                               t   t          t                      t                                                 t                                 t      t        t           \n",
            "     t           e         t                   t   t e     e  t   e                  t e                       t      t       t     e  t   e                 e        e  t      t        t    t e    \n",
            "     t  t    e   e e       t                   t   t e     e  t e e e   t  t    e  t t e e e  t       t    t   t     et    e  t   t e  t e e e   t  t    e   e e    t e  t      t        t    t e  t \n",
            "     t  t    e   e e     t t      t       t    t   t e     e  t e e e   t  t    e  t t e e e  t    t  t    t   t     et  t e  t   t e  t e e e   t  t    e   e e    t e  t  t   t        t    t e  t \n",
            "     t  t  a e t e e     t t      t       t    t   t e   t e  t e e e   t  t  a e  t t e e    t    t  t    t   t   e et  t e  t   t e  t e e e   t  t  a e t e e    t e  t  t   t     t  t  t t e  t \n",
            "     t  t  a e t e e t   t t  t   t       t    t   t e   t e  t e e e   t  t  a e  t t e e t  t    t  t    t   t   t et  t e  t   t e  t e e e   t  t  a e t e e t  t e  t  t   t     t  t  t t e  t \n",
            "     t  tt a e t e e t   t t  t   t e     t    t   t e   t e  t e e e   t  tt a e  t t e e t  t    t  t    t t t   t et  t e  t   t e  t e e e   t  tt a e t e e t  t e  t  ta  t     t  t  t t e  t \n",
            "     t  tt a e t e e t   t t  t   t e     t    t   t e   t e  t e e e   t  tt a e  t t e e t       t  t    t t t   t et  t e  t   t e  t e e e   t  tt a e t e e t    et t  ta  t     ta t  t t e  t \n",
            "     ta lt a e t e e t   t t  t   t e   t t    th  t e   t e  t e t e   ta tt a e  t the e t       t  t    t t t   t et  t e  t   t e  t e t e   ta tt a e t e e t    et tt ta  t     ta t  t the  t \n",
            "     ta lt ahe t e e ta  t t  t   t e   t t t  th  ther  the  t e t e   ta lt ahe  t the e t       t  t    tht th  t et  t e  t   t er t e t e   ta lt ahe t e e t    et tt tat t     ta t  t the  t \n",
            "     ta lt ahe the e th  t t  t   the   t t t  th  ther  thet the the   ta lt ahe  t the e th t    t  t    tht th  t et  t er t   t er the the   ta lt ahe the e th t et tt tat th    ta t  l ther t \n",
            "     ta lt ahe the e th  t t  t   the   t tht  th  ther  thet the the   ta lt ahe  t the e th t    tt th   tht th  e et  t er th  thet the the   ta lt ahe the e th t et tt tht th  l ta t  l thet   \n",
            "   k ta lt ahe the e th  t t  t   the   t tht  th  thet  thet the ehe k ta lt ahe lt the e th th   tt th   tht th  e et  t et th  thet the ehe k ta lt ahe the e th thet tt tet tr  l ta th l thet   \n",
            "   k ta lt ahe the e to  trt  t   the   t tht  th  thet  thet the ehetk ta lt ahe lt the e th th   tt th   tht th  e et  t et th  thet the ehetk ta lt ahe the e th thet tt  et tr  l th th l thet   \n",
            "   k ta lt are there to  trt  t   are   t tht  th  thetk thet the ehetk tatlt are kt the e th th   tt th   tht tht e et  t et th  thet the ehetk tatlt are there th thet tt  et tr  k th ta l thet   \n",
            "   k ta lt are there to  trt  t   arl   t thtt th  thetk thet the eretk tatlt are kt the e th th    t th   tht tht e et  t et thl thet the eretk tatlt are there th thet tt  et trt k th ta l thet   \n",
            "   k tallt are there to  trt  t   arl   t thtt th  thetk thet the eretk tallt are kt the e th t     t th   tht tet e et  t et tal thet the eretk tallt are there th thet tt  et trt y th ta l thetk  \n",
            "   k talls are the e to  art  t   arl   t trtt th  thetk thet therbretk talls are kt the e th  a    t ah   brt tet e et  t er tal thet the bretk talls are the e th  het tt  et trt y ta ta l thetk  \n",
            " e k talls are the e to  art  t   arl   t trtt th  therk thet therbresk talls are lt there th  a   kt ah   brt tet eretn ther tal thet therbresk talls are the e th  het ts  et trt y ta ta l thetkt \n",
            " e k talls are the e to  art  t   arl   t trtt to  thenk thet therbresk talls are lt there th  a   kt ar   brt tet eretn ther tal thet therbresk talls are the e th  aet tt  at trt y ta ta s thetkt \n",
            " etk talls are the e ta  arte t   arl   t trst to  thenk thet therbresk talls are lt there th  a   tt art  brt tat eretn ther tal thet therbresk talls are the e th  aet tt  at twl y ta tals thetkt \n",
            " etk talls are there ta  arte t   arl k t trst to  thenk thet therbreck talls are lt there th  ar  tt art  brt tat eretn ther tal thet therbreck talls are there th  het tt  ot trl y ta tals thetkt \n",
            " etk talls are there ta  arte t   arl k t trst to  thenk thet therbreck talls are lt there th  ar  tt art  brt  et eretn ther tal thet therbreck talls are there th  het tt  ot brl y ta tals thenkt \n",
            "retk talls are there ton arte t   ard k t trlt to  thenk thet therbreck talls are lt there th tark tt art  brs  et eretn ther tal thet therbreck talls are there th thet ts aot brl y ta tals thenkt \n",
            "retk talls are there ton arte t   ard k t brlt to  thenk thet therbreck talls are lt there th ta k tt art  brs  et eretn then tal thet therbreck talls are there th thet ts aot brlly ta tals thenkt \n",
            "renk talls are there ton arte s   ard k t brlt to  thenk thet therbreck talls are kt there th ta k tt art  brs  et eretn then tal thet therbreck talls are there th thet ts aot brlly ta tals thenkt \n",
            "renk talls are there ton arte s   ard k t brlt tot thenk thet therbreck talls are kt there th ta k tt art  brs  et eretn then tal thet the breck talls are there th thet tt aot brlly ta tals thenkt \n",
            "renk talls are there ton arte so  ard kht bust tot thenk thet therbreck talls are kt there th th k tt ort  brs ret eretn then tal thet therbreck talls are there th thet ut aot bally ta tall thenkts\n",
            "renk talls are there ton arte so  ard kht bust tot thenk thet therbreck talls are kt there th th k tt ort  brs ret eretn then tal thet therbreck talls are there th toet us aot badll ta tanl thenkts\n",
            "renk talls are there ton arte so  ard kht bult tot thenk thet therbreck talls are kt there th th k ts ort  brs ret eretn then tan thet therbreck talls are there th toet us oot badll ta tanl thenkts\n",
            "renk talls are there ton arte so  ard yht bult tot thenk thet therbreck talls are kt there th th k ts ort  brs rat eretn then tan thet therbreck talls are there th thet us oot badll ta tant thenkts\n",
            "reck walls are there ton arte so  ard yht bust tot thenk thet therbreck walls are lt there th thek ts ort  brs tat eretn then tal thet the breck walls are there th thet us oot badll ta tant thenkts\n",
            "reck walls are there ton arte so  ard yht bust tot thenk thet the breck walls are lt there th thek ts out  brt tet eretn then wal thet the breck walls are there th thet us oot uadll ta tant thenkts\n",
            "reck walls are there ton arte so  ard yht bust uot thenk thet the brick walls are kt there th thek us outk brt tet er in thin wal thet the brick walls are there to thet us oot uadll ta tant thenkts\n",
            "reck walls are there tor arte so  ard yht bust rot thenk thet the brick walls are kt there th thek us outk brt tet er in thin way whet the brick walls are there to thet us oot uadly ta tant thenkts\n",
            "reck walls are there tor arte so  and yht bust rot thenk thet the brick walls are kt there to thek us outk brt ret er in thin way whet the brick walls are there to thet us oot uadly ta tant thenkts\n",
            "reck walls are there tor arte so  and yhu bust rot theck thet the brick walls are 't there to teek us outk brt rether in thin way whet the brick walls are there to thet us oot badly ta tant theckss\n",
            "reck walls are there tor arte son and you bust rot theck thet the brick walls are 't there to teek us outk brt rether in thin way whet the brick walls are there to thet us oot badly ta tant theckss\n",
            "reck walls are there tor arte son and you bust rot theck thet the brick walls are 't there to teep us out  brt rether in thin way whet the brick walls are there to thew us oot badly ta tant thecksk\n",
            "rick walls are there tor arteason and you bust rot theck thet the brick walls are 't there to teep us out  brt rather in this way whit the brick walls are there to thew us oot badly ta tant thenksk\n",
            "rick walls are there tor a teason and you busthrot theck thet the brick walls are 't there to keep us outk brt rather in this way whit the brick walls are there to khew us hot badly ta tant thenksk\n",
            "rick walls are there tor a teason and you busthrot theck thet the brick walls are 't there to keep us outk but rather in this way whit the brick walls are there to khew us hot badly ta tant thenksk\n",
            "rick walls are there tor a teason and you busthrot thenk thet the brick walls are 't there to keep us outk but rather in this way whit the brick walls are there to khow us how badly ta want thenksk\n",
            "rick walls are there tor a teason and you busthrot thenk thet the brick walls are 't there to keep us outk but rather in this way whit the brick walls are there to khow us how uadly ta want thenksk\n",
            "rick walls are there tor a teason and you bust rot thenk thet the brick walls are 't there to keep us outk but rather in this way thit the brick walls are there to khow us how uadly ta want thenksk\n",
            "rick walls are there tor a teason and you bust rot thenk thet the brick walls are 't there to keep us outk but rather in this way thit the brick walls are there to khow us how uadly ta want thenksk\n",
            "rick walls are there tor a teason and you bust not thenk that the brick walls are 't there to keep us out  but rather in this wal thit the brick walls are there to khow us how uadly ta want thenksk\n",
            "rick walls are there tor a teason and you bust not thenk that the brick walls are 't there to keep us outh but rather in this wal thit the brick walls are there to khow us how uadly ta want thenksy\n",
            "rick walls are there tor a teason and you bust not thenk that the brick walls are 't there to keep us outh but rather in this wal thit the brick walls are there to khow us how uadly ta want thinksy\n",
            "rick walls are there tor a teason and you bust not thenk that the brick walls are 't there to keep us out, but rather in this wal thit the brick walls are there to khow us how uadly wa want thenksk\n",
            "rick walls are there tor a teason and you bust not think that the brick walls are 't there to keep us out, but rather in this way thit the brick walls are there to khow us how badly wa want thenksk\n",
            "rick walls are there tor a teason and you bust not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly wa want thinksk\n",
            "rick walls are there tor a teason and you bust not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly wa want thinksk\n",
            "rick walls are there tor a teason and you bust not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a teason and you bust not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksk\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinks.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinks.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinks.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinks.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinks.\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "8qUkbiw2t0Il",
        "outputId": "7aa4275f-3484-4330-8f0e-4706d534b61c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinks.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과가 어떤가요?? 마지막 에폭의 문장이 그럴싸한가요?"
      ],
      "metadata": {
        "id": "PkIzDTdyvTHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task2\n",
        "\n",
        "위 sentence는 제가 임의로 생성한 문장들입니다.\n",
        "\n",
        "마음에 드시는 문구 가져오셔서 문장이 어떻게 생성되는지 확인해보세요! \n",
        "\n",
        "영어가 아닌 한국어로 시도해보는 것도 좋겠죠? \n",
        "\n",
        "수정이 많이 필요(토큰화 등) 할 수 있으나 한번 시도해보시는 것 권장드립니다 :)\n",
        "\n",
        "위 베이스라인은 어디든 수정하셔도 좋고 조금 더 자연스러운 문장이 나올 수 있게 다양한 시도를 해보세요!\n",
        "\n",
        "조건 : 문장 3개 이상, 연결성이 있는 문장을 \" \" 으로 구분하여 ( )에 넣기"
      ],
      "metadata": {
        "id": "kN1zL8Dpvane"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 첫 번째"
      ],
      "metadata": {
        "id": "PA88P4dFqZl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "IKp-lKrjvXR9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"And I can take you for a ride\"\n",
        "            \"I had a premonition that we fell into a rhythm\"\n",
        "            \"Where the music don't stop for life\"\n",
        "            )"
      ],
      "metadata": {
        "id": "CkOCxUMGXHBb"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sentence))  # 중복 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩"
      ],
      "metadata": {
        "id": "VOkj3AMlXvyc"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(char_dic) # 공백도 여기서는 하나의 원소"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mkAM-EpX5x7",
        "outputId": "5e6ab744-7d44-47ed-ec6c-8558fac6adca"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'W': 0, 'I': 1, 'a': 2, 'u': 3, 'e': 4, 'i': 5, 'p': 6, 'f': 7, 'm': 8, 'h': 9, 't': 10, 'r': 11, 'y': 12, 'k': 13, 'd': 14, ' ': 15, 'w': 16, 'l': 17, 'A': 18, 'o': 19, 's': 20, \"'\": 21, 'c': 22, 'n': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IegukDYQX9qP",
        "outputId": "a452cadd-8ce1-4e47-d06e-ad7319630bda"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "LTVVKpQGdZpO"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "  x_str = sentence[i : (i+sequence_length)]\n",
        "  y_str = sentence[i+1 : (i+sequence_length+1)]\n",
        "  print(i, x_str, '->', y_str)\n",
        "  \n",
        "  x_data.append([char_dic[c] for c in x_str]) # x str to index\n",
        "  y_data.append([char_dic[d] for d in y_str]) # y str to index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vKyUhksdjJe",
        "outputId": "68d62494-5faf-4bcc-9a56-4660ae5c95f9"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 And I can  -> nd I can t\n",
            "1 nd I can t -> d I can ta\n",
            "2 d I can ta ->  I can tak\n",
            "3  I can tak -> I can take\n",
            "4 I can take ->  can take \n",
            "5  can take  -> can take y\n",
            "6 can take y -> an take yo\n",
            "7 an take yo -> n take you\n",
            "8 n take you ->  take you \n",
            "9  take you  -> take you f\n",
            "10 take you f -> ake you fo\n",
            "11 ake you fo -> ke you for\n",
            "12 ke you for -> e you for \n",
            "13 e you for  ->  you for a\n",
            "14  you for a -> you for a \n",
            "15 you for a  -> ou for a r\n",
            "16 ou for a r -> u for a ri\n",
            "17 u for a ri ->  for a rid\n",
            "18  for a rid -> for a ride\n",
            "19 for a ride -> or a rideI\n",
            "20 or a rideI -> r a rideI \n",
            "21 r a rideI  ->  a rideI h\n",
            "22  a rideI h -> a rideI ha\n",
            "23 a rideI ha ->  rideI had\n",
            "24  rideI had -> rideI had \n",
            "25 rideI had  -> ideI had a\n",
            "26 ideI had a -> deI had a \n",
            "27 deI had a  -> eI had a p\n",
            "28 eI had a p -> I had a pr\n",
            "29 I had a pr ->  had a pre\n",
            "30  had a pre -> had a prem\n",
            "31 had a prem -> ad a premo\n",
            "32 ad a premo -> d a premon\n",
            "33 d a premon ->  a premoni\n",
            "34  a premoni -> a premonit\n",
            "35 a premonit ->  premoniti\n",
            "36  premoniti -> premonitio\n",
            "37 premonitio -> remonition\n",
            "38 remonition -> emonition \n",
            "39 emonition  -> monition t\n",
            "40 monition t -> onition th\n",
            "41 onition th -> nition tha\n",
            "42 nition tha -> ition that\n",
            "43 ition that -> tion that \n",
            "44 tion that  -> ion that w\n",
            "45 ion that w -> on that we\n",
            "46 on that we -> n that we \n",
            "47 n that we  ->  that we f\n",
            "48  that we f -> that we fe\n",
            "49 that we fe -> hat we fel\n",
            "50 hat we fel -> at we fell\n",
            "51 at we fell -> t we fell \n",
            "52 t we fell  ->  we fell i\n",
            "53  we fell i -> we fell in\n",
            "54 we fell in -> e fell int\n",
            "55 e fell int ->  fell into\n",
            "56  fell into -> fell into \n",
            "57 fell into  -> ell into a\n",
            "58 ell into a -> ll into a \n",
            "59 ll into a  -> l into a r\n",
            "60 l into a r ->  into a rh\n",
            "61  into a rh -> into a rhy\n",
            "62 into a rhy -> nto a rhyt\n",
            "63 nto a rhyt -> to a rhyth\n",
            "64 to a rhyth -> o a rhythm\n",
            "65 o a rhythm ->  a rhythmW\n",
            "66  a rhythmW -> a rhythmWh\n",
            "67 a rhythmWh ->  rhythmWhe\n",
            "68  rhythmWhe -> rhythmWher\n",
            "69 rhythmWher -> hythmWhere\n",
            "70 hythmWhere -> ythmWhere \n",
            "71 ythmWhere  -> thmWhere t\n",
            "72 thmWhere t -> hmWhere th\n",
            "73 hmWhere th -> mWhere the\n",
            "74 mWhere the -> Where the \n",
            "75 Where the  -> here the m\n",
            "76 here the m -> ere the mu\n",
            "77 ere the mu -> re the mus\n",
            "78 re the mus -> e the musi\n",
            "79 e the musi ->  the music\n",
            "80  the music -> the music \n",
            "81 the music  -> he music d\n",
            "82 he music d -> e music do\n",
            "83 e music do ->  music don\n",
            "84  music don -> music don'\n",
            "85 music don' -> usic don't\n",
            "86 usic don't -> sic don't \n",
            "87 sic don't  -> ic don't s\n",
            "88 ic don't s -> c don't st\n",
            "89 c don't st ->  don't sto\n",
            "90  don't sto -> don't stop\n",
            "91 don't stop -> on't stop \n",
            "92 on't stop  -> n't stop f\n",
            "93 n't stop f -> 't stop fo\n",
            "94 't stop fo -> t stop for\n",
            "95 t stop for ->  stop for \n",
            "96  stop for  -> stop for l\n",
            "97 stop for l -> top for li\n",
            "98 top for li -> op for lif\n",
            "99 op for lif -> p for life\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14ubdIzAlw1y",
        "outputId": "e3c05c17-a2f7-4077-f927-e706b597795c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18, 23, 14, 15, 1, 15, 22, 2, 23, 15]\n",
            "[23, 14, 15, 1, 15, 22, 2, 23, 15, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x data를 one-hot encoding\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "NIkONBydl7Rk"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzEtmsCOmISM",
        "outputId": "93b3a3fa-06b7-4d1d-aef0-bddd610e3e94"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([100, 10, 24])\n",
            "레이블의 크기 : torch.Size([100, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev8sd4ykmNnt",
        "outputId": "bf881210-2886-46a2-f665-8a60c0e3b39f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COUB8iQgmPEv",
        "outputId": "a5b12859-38bb-47be-c7db-8b8575f0e42e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([23, 14, 15,  1, 15, 22,  2, 23, 15, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):  # 현재 hidden_size는 dic_size와 같음\n",
        "      super(Net, self).__init__()\n",
        "      self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "      self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "j00opHBGmSf7"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 두 개의 층"
      ],
      "metadata": {
        "id": "HCBOU-axmvu9"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "Zdz0Uhd-mxsd"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape)  # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "823FpHHFm7L-",
        "outputId": "fc712149-8f97-4a42-a1f2-0af0ee99ce86"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 10, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P46E2rnVm-JZ",
        "outputId": "0a3d9735-883e-4a05-8082-f916546d958a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W_cUA0-owD-",
        "outputId": "ea21ed05-a8d3-451b-8063-14395b50b098"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 10])\n",
            "torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X)\n",
        "  loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  results = outputs.argmax(dim=1)\n",
        "  predict_str = \"\"\n",
        "  for j, result in enumerate(results):\n",
        "    if j == 0:\n",
        "      predict_str += ''.join([char_set[t] for t in result])\n",
        "    else:\n",
        "      predict_str += char_set[result[-1]]\n",
        "  \n",
        "  print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iulgUcrQoxza",
        "outputId": "cbe95413-fefe-44c3-9964-a8890bb8a518"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uWWIfuWmaWephueiiuIeumeeuuaIufpieuuhmfpihmfpieupieuhmfpihmfpieuaeuaIaIpieuafhmfphmfpieuaIumfpihufpieuihmfpieuaIieuhmfpieuaI\n",
            "eapaIWeiWWWuWfefaWWWeiWpieuhmfhhmfphmfpieuaimfhhmfphmhmhmfpiehmfpieuahmfphhmfpipieepihmfphmhhmfpiemfphhhmfpieupieuhmfhhmfpi\n",
            "iihmfIpWppWWiWppIWiapiWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW\n",
            "mmpIpummephhimeueImpemmhiehiehiehmmiieuhimmiimimieihiemimhmmeieuihmmuihemhhmfhiememimWemhimuheeiiemmfhimuiimuimWimhhmuiiemh\n",
            "iiuiIfiuieuIuiifihifiiipppWIupppufpiIuuWWppWpWpppuuhIpupIWpupfIpWhufpWhuppWuufIupIupuufpWWppfpupupuupphuufIpupppWWpWpuuuuuW\n",
            "ihhpIIhiimuWieiIhipIhehWeIWmWmiIWauWIWWWuWIWuWWWiWuIIWuWIWmfuifuWIWapWIWapiaWhmfaIiWfWIaWfWpiIuafpapImIWfaIWuWuWmWIfIfafWaW\n",
            "uhemIhfhupIIueiIhupIieumIIfmfpiIufuWIuiWufIWmfmfmWppmfIfmfmfuifpiImfuiIuipufufmufpiiummfWmIuiIuffpifpiIuiIIfIWuhmfIipifuuiW\n",
            "uueaIefhupmIueiIuIpaieimIphmfpieufIiIuiWufIWmfpfmuphmfpfpfmfhmfpihmfpieuiIufuihmfpimuWmfumIpieuifpmfpieuiIIWIWuhmfpiIuiuuiW\n",
            "ufifWefhmeIIueimuIeaiehIaphmfpieufIWIuuWuWIWmpIuIupeIupieupuhmfpieufpieuihmfpieufpppuuIfuiehmepieImfpieuiIpaIuIhmfpifuuuuiW\n",
            "ufifWefhaeuIfefmuIeafehIIihmfpieupIWupuWuaIWuaIuIpIhmfIieuIfhmfIieuapWeuuhmfpieuapWWuhIeuaIhmmfieImfpieuaIhaImIWmehieWifuaW\n",
            "uuifIpfiaehIuefmuIeafeiIpihmfpieupIWupWWuaIWfpIeuWIemfIhIuIihmfpieufpieuuImfpieuuIWWuWIeWeIhmmpieIhfpieuaIieIaIWieIWIpuupiW\n",
            "uuiuaefiaehIiefmiIuaueiIIihmfpieuaIWuiWWuaIWfpIeuWIemfIhmupuhmfpWeufpWeuiImfpieuupWWuWIeWaIhmfpiehmfpieufIimIaIWueIWIpmuuiW\n",
            "uuiuaefiaehuuefmiIuauehfpihmfIieuhmWuiWWuaIWfpieuWIeuaIhmfpihmfpWeufpWeuaImfpWeuupWWuWIeWIIhmfpieumIpieufIimIaIWufIWIWmfuiW\n",
            "uuhuaefhIehuiefmiuuafeifpieuaIWmuhmWuiiWuaIWupWmuWieuaIhmfpihmfIWemfpWeuaImfpWeuieWWuWIeWmWhmfuieumIpieufIhmIpiWuaIWIWmfpiW\n",
            "uuhpaefhIehfiefmifihfeifpieuaIWmfpWWuaiWuaIWupWmfWiemaIhmfpiemfIWeufpWeuaImfhWfpieWWfWIeWmWhieuieumIpieufIhmIpiWuaIWIWefpiW\n",
            "iuheaefiIehfiefmifihfeifpieuaIWmfpWWuaiWuaIWuIWmfWihmfIhmfpieufIWeufIWeuiIWfhWfpieWWfWIeWmWhieuieumIhieufIhmIpiWuaIWIWeupiW\n",
            "iupeapfiIphfiefmifihieifpieuaIWmfpWWuaIWuaIWuaWmfWahmfIhmfpieufIWeufIWeuiIWfhWfpifWWeWIeWmWhieWiIumIhiemfphmfpieuaIWIWeupiW\n",
            "fupeapfhIphfiefmifiaimifpieuaIWmfpWWuaIWuaIWuaWmuWiemaIhmfpieufIWeueIWeuiIWfhWfpieWWfWIeWmWhieWiIumIhihmfphmfpieuaIWIWmfpiW\n",
            "iupWapfhIhhfiefmiuiifeifpieuaIhmfpieuaIWapIWuaWmuWihmfIhmfpieufIWeueWWeuiIWfhmfpieWWfWIeWmWhieWiIumhhmhmfehmfpieuaIWIWefpiW\n",
            "uupuapfhmhhfiefmiuuiueifpieuaIhmfpieuaIWapIWupWmuWiemfIhmfpieufIWeuaWWeuiIWIhmfpieuWfWeeWmWpieWiIumhhmhmfphmfpiWuaIWIWefpiW\n",
            "fupfapfhaphuiefmiuiiumifpieuaIhmfpieuaIWapIWupWmuWihmfIhmfpieufpieuaWWeuiIWIhmfpieuafWmeWmWpieWiIuihmfhmfphmfpiWuaIWIWepiIW\n",
            "fupfapfhmhhIiefmiuiaumifpieuaIWmfpWeuiIWapIWuIWmuWihmfIhmfpieufpieuaWWeuiIWahmfpieuaIWmeWmWpieWaIWihmfhmfphmfpieuaIaIWepihW\n",
            "fupfapIhahhIieImiIiaumifpieuaIWmfpWepiIWapiWuIWmfWihmfIhmfpieufpieuaWWeuaIWahmfpieuaIWeeWmWpieuaIpihmfhmfphmfpieuaIaIWfpihW\n",
            "fupfapIhIhfIieImiIiaumifpieuaIhmfpiepiIWapiWuIWmfWihmfIhmfpieufpieuaWWeuiIWahmfpieuaIWeeWmfpieuaIpihhmhmfphmfpieuaIaIWfpihW\n",
            "fupfapIhIhhIieImiIiaumifpieuaIhmfpieuiIWapiWuIhmfWieufIhmfpieufpieuaWWIuiIWahmfpieuaIWeeWmfpieuaIpiehmhmfphmfpieuaIaIWepieW\n",
            "fupfapIiuhhIieImiIiaumifpieuaIhmfpieuaIWupiWuphmfWihmfIhmfpieufpieuaWWIuiIWahmfpieuaIWmeWmfpieuaIpiehmhmfphmfpieuaIaIWefpiW\n",
            "fupfapIiuhhIieImiuiaumifpieuaIhmfpieuaIWupiWuphmfWihmfIhmfpieufpieuaWWeuiIWahmfpieuaIWmeWmWpieuaIpiehmhmfphmfpieuaIaIWefpiW\n",
            "fupfapIiuhhIieImiuiaumifpieuaIhmfpieuaIWapiWuphmfWihmfIhmfpieufpieuaWWeuiIWahmfpieuaIWaeWmWpieuaIpiIhmhmfphmfpieuaIaIWeupiW\n",
            "fupfapIiuhhIieImiuiaumifpieuaIhmfpieuaIWapiWuphmfWihmfIhmfpieufpieuaIWeuiIWahmfpieuaIWaeWmWpieuaIpiIhmhmfphmfpieuaIaIWeupiW\n",
            "fupfaeIiuhhIieImiuiaumifpieuaIhmfpieuaIWapiWuphmfWihmfIhmfpieufpieuaIWeuiIWahmfpieuaIWaeWmWpieuaIpiIhmhmfphmfpieuaIaIWeupiW\n",
            "fupfaeIiuhhIieImiuiaumifpieuaIhmfpieuaIWamiWuehmfWihmfIhmfpieufpieuaIWeuiIWahmfpieuaIWmeWmWpieuaIpiIhmhmfphmfpieuaIaIWeuIhW\n",
            "fupfaeIiIhhIieImiuiaumifpieuaIhmfpieuaIWamiWuphmfWihufIhmfpieufpieuaIWeuiIuahmfpieuaIWIeWmWpieuaIpmIhmhmfphmfpieuaIaIWeuIhW\n",
            "fupfaeIiIhhIieImiuiaumifpieuaIhmfpieuaIWamiWuphmfWihufIhmfpieufpieuaIWfuiIuahmfpieuaIWIeWmWpieuaIhmIhmhmfphmfpieuaIaIWeuIhW\n",
            "fupfaeIiIhhIieImiuiaumifpieuaIhmfpieuaIWamfWuehmfWihuaIhmfpieufpieuaImfuiIuahmfpieuaIWIeWmWpieuaIpiehmhmfphmfpieuaIaIWeuIhW\n",
            "fupfaeIiIhhIieImiuiaumifpieuaIhmfpieuaIWamfWuehmfWieuaIhmfpieufIieuaImfuiIuahmfpieuaIWIeWmWpieuaIpiehmhmfphmfpieuaIfIWeuihW\n",
            "fupfaeIiIhhIieImiuiaumifpieuaIhmfpieuaIWamfWuehmfWieuaIhmfpieufIieuaImfpiIuahmfpieuaIWIeWmWpieuaIpiehmhmfphmfpieuaIfIWeuihW\n",
            "fupfaeIiIhhIieImiuiaumifpieuaIhmfpieuaIWamfWuehmfWieuaIhmfpieufIieuaImfpiIuahmfpieuaIWIeWmWpieuaIpiehmhmfphmfpieuaIWIWeuihW\n",
            "fupuaeIiIhhIieImiuiaumifpieuaIhmfpieuaIWamhWuphmfWihuaIhmfpieufIieuaImfpiIuahmfpieuaIWIeWmWpieuaIpiehmhmfphmfpieuaIWIWeuihW\n",
            "fupuaeIiIhhIieImiuiaumifpieuaIhmfpieuaIWamfWuphmfWihuaIhmfpieufIieuaWWfpiIuahmfpieuaIWIeWmWpieuaIpiehmhmfphmfpieuaIaIWeumfW\n",
            "fupuaeIiIhhIifImiuiiumifpieuaIhmfpieuaIWamhWiphmfWieufIhmfpieufIieuaWWfpiIuahmfpieuaIWIfWmWpieuaIpiehmhmfphmfpieuaIWIWeuieW\n",
            "fupuaefiIhhIifImiuiiumifpieuaIhmfpieuaIWamfWiphmfWieufIhmfpieuaIieuaWWfpiIuahmfpieuaIWIfWmWpiehaIpiehmhmfphmfpieuaIWIWeuieW\n",
            "fupuaefiIhhIifImiuiiumiWpieuaIhmfpieuaIWamfWiphmfWieufIhmfpieuaIieuaWWfpiIuahmfpieuaIWIfWmWpieuaIpmehmhmfphmfpieuaIWIWeuieW\n",
            "fupuaefiIhhIifImiuiiumiWpieuaIhmfpieuaIWhmfWiphmfWieufIhmfpieufpieuaWWfpiIuahmfpieuaIWIfWmWpieuaIpmehmhmfphmfpieuaIWIWeumfW\n",
            "fupuapfiIhhIifImiuiiumiWpieuaIhmfpieuaIWamhWuphmfWieufIhmfpihufpieuaIWeuiIWahmfpieuaIWIfWmWpieuaIpmehmhmfphmfpieuaIWIWeumhW\n",
            "fupuapfiIhhIifImiuiiumiWpieuaIhmfpieuaIWamhWuphmfWieuaIhmfpihmfpieuaIWeuiIWahmfpieuaIWIfWmWpieuaIhmehmhmfphmfpieuaIWIWeumhW\n",
            "fupfapfiIhhIifImiuiiumiWpieuaIhmfpieuaIWamhWuphmfWieuaIhmfpihmfpieuaIWfuiIWahmfpieuaIWIfWmWpieuaIhmfhmhmfphmfpieuaIWIWeuihW\n",
            "fupfaefiIhhIifImiuiWumiWpieuaIhmfpieuaIWapiWuphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWpieuaIpmfhmhmfphmfpieuaIWIWeuihW\n",
            "fupfaefiuhhIifImiuiWumiWpieuaIhmfpieuaIWapiWuphmfWieuaIhmfpihmfpieuaIWepiIWahmfpieuaIWIeWmWpieuaIpiehmhmfphmfpieuaIWIWeuihW\n",
            "fupfaefiuhhIifImiuiiumifpieuaIhmfpieuaIWapiWfphmfWieuaIhmfpihmfpieuaIWeuiIWahmfpieuaIWIeWmWpieuaIpiehmhmfphmfpieuaIWIWeuihW\n",
            "fupfaefiuhhIifImiuiiumifpieuaIhmfpieuaIWaphWfphmfWieuaIhmfpihmfpieuaIWepiIWahmfpieuaIWIeWmWpieuaIpifhmhmfphmfpieuaIWIWeuieW\n",
            "fupfaefiuhhIifImiuiiumifpieuaIhmfpieuaIWaphWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWpieuaIpmfpmhmfphmfpieuaIWIWeuieW\n",
            "fupfaefiuhhIifImiuiWumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWpiehaIpmfpihmfphmfpieuaIWIWeuihW\n",
            "fupfaefiuhhIifImiuiWumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWpiehaIpmfpihmfphmfpieuaIWIWeuihW\n",
            "fupfaefiuhhIifImiuiWumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWpiehaIpmfpihmfphmfpieuaIWIWeuihW\n",
            "fupuaefiihhIifImiuiWumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWpiehaIpmfpihmfphmfpieuaIWIWeuieW\n",
            "fupuaefiihhIifImiuiWumifpieuaIhmfpieuaIWaphWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWpiehaIpmfpihmfphmfpieuaIaIWeuieW\n",
            "fupuaefiihhIifImiuiWumifpieuaIhmfpieuaIWaphWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWpiehaIpmfpihmfphmfpieuaIaIWeuieW\n",
            "fupuaefiihhIifImiuiWumifpieuaIhmfpieuaIWaphWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWpiehaIpmfpihmfphmfpieuaIaIWeuieW\n",
            "fupuaefiihhIifImiuiiumifpieuaIhmfpieuaIWaphWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWpiehaIpmfpihmfphmfpieuaIaIWeuieW\n",
            "fupuaefiihhIifImiuiiumifpieuaIhmfpieuaIWaphWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWmiphaIpmfpmhmfphmfpieuaIWIWeuieW\n",
            "fupuaefiihhIifImiuiiumifpieuaIhmfpieuaIWaphWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWmiphaIpmfpihmfphmfpieuaIaIWeuieW\n",
            "fupuaefiihhIifImiuiWumifpieuaIhmfpieuaIWaphWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWmiphaIpmfpihmfphmfpieuaIaIWeuieW\n",
            "fupuaefiihhIifImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWmfphaIpmfpihmfphmfpieuaIWIWeuieh\n",
            "fupuaefiihhIifImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIeWmWmfphaIpmIpihmfphmfpieuaIWIWeuieh\n",
            "fupuaefiihhuifImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfphaIpmIpihmfphmfpieuaIWIWeuieh\n",
            "fupfaefiihhuifImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfphaIpmIpmhmfphmfpieuaIWIWeuieh\n",
            "fupfaefiihhuifImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfphaIpIIpmhmfphmfpieuaIWIWeuieh\n",
            "fupfaefiihhuipImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfphaIpIIpmhmfphmfpieuaIWIWeuieh\n",
            "fupfaefiihhuipImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpiaIpIIhmhmfphmfpieuaIWIWeuieh\n",
            "fupfWefiihhuipImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpiaIpIIhmhmfphmfpieuaIWIWeuieh\n",
            "aupfWefiihhuipImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfphaIpIIhmhmfphmfpieuaIaIWeuieh\n",
            "aupfWefiihhuipImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpiaIpIIhmhmfphmfpieuaIWIWeuieh\n",
            "aupfWefiihhuipImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpiaIpIIhmhmfphmfpieuaIWIWeuieh\n",
            "aupfWefiihhuipImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpuaIpIIhmhmfphmfpieuaIaIWeuieh\n",
            "aupfWefiihhuipImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpiaIpIIhmhmfphmfpieuaIWIWeuaeh\n",
            "aupfWefiihhuipImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpuaIpIIumhmfphmfpieuaIWIWeuaeh\n",
            "aupfWefiihhuipImuuiWumifpieuaIhmfpieuaIWahhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpuaIpIIumhmfphmfpieuaIWIWeuaeW\n",
            "aupfWefiihhIipImuuiWumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpuaIpIIumhmfphmfpieuaIWIWeuaeW\n",
            "aupfWefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpuaIpIIumhmfphmfpieuaIWIWeuaeW\n",
            "aupffefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpuaIpIIumhmfphmfpieuaIWIWeuaeW\n",
            "aupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpuaIpIIumhmfphmfpieuaIaIWeuaeW\n",
            "aupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmfpuaIpIIumhmfphmfpieuaIaIWeuaeW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWpipuaIpIIumhmfphmfpieuaIWIWeuaeW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWpipuaIpIIumhmfphmfpieuaIaIWeuaeW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWpipuaIpIIumhmfphmfpieuaIaIWeuaeW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfphmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWpieuaIpIIumhmfphmfpieuaIWIWeuaeW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWpieuaIpIIumhmfphmfpieuaIWIWeuaeW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWpieuaIpIIumhmfphmfpieuaIaIWeuaeW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmipuaIpIIumhmfphmfpieuaIWIWeuaeW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWpieuaIpIIumhmfphmfpieuaIWIWeuahW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmipuaIpIIumhmfphmfpieuaIWIWeuahW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWpieuaIpIIumhmfphmfpieuaIaIWeuahW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmipuaIpIIumhmfphmfpieuaIWIWeuahW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmipuaIpIIumhmfphmfpieuaIWIWeuahW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmipuaIpIIumhmfphmfpieuaIWIWeuahW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmipuaIpIIumhmfphmfpieuaIWIWeuahW\n",
            "fupfaefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmipuaIpIIumhmfphmfpieuaIWIWeuahW\n",
            "fupffefiihhIimImuuiiumifpieuaIhmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmipuaIpIIumhmfphmfpieuaIWIWeuahW\n",
            "fupffefiihhIimImuuiiumifpieuaIWmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmipuaIpIIumhmfphmfpieuaIWIWeuahW\n",
            "aupffefiihhIimImuuiiumifpieuaIWmfpieuaIWamhWfpWmfWieuaIhmfpihmfpieuaIWfpiIWahmfpieuaIWIuWmWmipuaIaIIumhmfphmfpieuaIWIWeuahW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 두 번째\n",
        "https://wikidocs.net/45101"
      ],
      "metadata": {
        "id": "X8mKB-zTqcvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "3DAjKQv4o6bM"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\"어터슨 변호사는 결코 환하게 웃는 법이 없는 묵뚝뚝한 표정을 가진 사람이었다\"\n",
        "        \"그는 야윈 데다가 키가 크고 무미건조하고 음울해 보이기까지 했다\"\n",
        "        \"그럼에도 어터슨에게는 어딘가 매력적인 부분이 있었다\")"
      ],
      "metadata": {
        "id": "o_XnRGhYqjfl"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기: {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0Fpb7r1tlkJ",
        "outputId": "f78ea75f-ed2d-4717-81db-cc0707932230"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q4FoDxatwG5",
        "outputId": "92dd55d5-c4ec-467d-d5f9-074e7b0acb38"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'어터슨': 1, '변호사는': 2, '결코': 3, '환하게': 4, '웃는': 5, '법이': 6, '없는': 7, '묵뚝뚝한': 8, '표정을': 9, '가진': 10, '사람이었다그는': 11, '야윈': 12, '데다가': 13, '키가': 14, '크고': 15, '무미건조하고': 16, '음울해': 17, '보이기까지': 18, '했다그럼에도': 19, '어터슨에게는': 20, '어딘가': 21, '매력적인': 22, '부분이': 23, '있었다': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = list()\n",
        "for line in text.split('\\n'):\n",
        "  encoded = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(encoded)):\n",
        "    sequence = encoded[:i+1]\n",
        "    sequences.append(sequence)\n",
        "\n",
        "print('학습에 사용할 샘플의 개수: {}'.format(len(sequences)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdFA7gZMtz_C",
        "outputId": "aa3f4ea9-d9da-422d-968a-0b72a7d4cabe"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습에 사용할 샘플의 개수: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3pW-5EXuGob",
        "outputId": "213f73b0-09b2-4804-9503-41799dcce2de"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2], [1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in sequences)  # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
        "print('샘플의 최대 길이 {}'.format(max_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNeuxwssuII4",
        "outputId": "dab85d37-c897-4714-d0a9-f2b710d6e0f4"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플의 최대 길이 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')"
      ],
      "metadata": {
        "id": "RlgE6rgeuaKk"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JViCwScguerc",
        "outputId": "7915e151-fc06-49e5-e31b-ef30f12e4477"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            " [ 0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
            " [ 0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
            " [ 0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n",
            " [ 0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
            " [ 0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            " [ 0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
            " [ 0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n",
            " [ 0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
            " [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]"
      ],
      "metadata": {
        "id": "-MHQX_3dufiM"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bQm2rY9umjr",
        "outputId": "c57a73de-d8e6-4b1d-9298-279e1689dc01"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
            " [ 0  0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
            " [ 0  0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
            " [ 0  0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n",
            " [ 0  0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
            " [ 0  0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            " [ 0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
            " [ 0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]\n",
            " [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]\n",
            " [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkDBpD6UunjB",
        "outputId": "d6e9c3dd-2b6b-4ec5-de9c-97ccf93955f4"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "rpkwj90auoha"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdvdL_csurL1",
        "outputId": "c9ff5c1d-311c-4662-8f05-5b8985a1e0e1"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"
      ],
      "metadata": {
        "id": "tiNdI9JfurvJ"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 10\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPqpY6CCuxTF",
        "outputId": "0678a960-cf44-402b-a382-c4fac302fbd4"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 1s - loss: 3.2271 - accuracy: 0.0435 - 1s/epoch - 1s/step\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 3.2088 - accuracy: 0.0870 - 14ms/epoch - 14ms/step\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 3.1917 - accuracy: 0.0870 - 11ms/epoch - 11ms/step\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 3.1753 - accuracy: 0.0870 - 13ms/epoch - 13ms/step\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 3.1593 - accuracy: 0.0870 - 12ms/epoch - 12ms/step\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 3.1432 - accuracy: 0.1739 - 11ms/epoch - 11ms/step\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 3.1267 - accuracy: 0.2174 - 17ms/epoch - 17ms/step\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 3.1096 - accuracy: 0.2174 - 10ms/epoch - 10ms/step\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 3.0918 - accuracy: 0.1304 - 11ms/epoch - 11ms/step\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 3.0731 - accuracy: 0.1304 - 10ms/epoch - 10ms/step\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 3.0534 - accuracy: 0.1304 - 11ms/epoch - 11ms/step\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 3.0326 - accuracy: 0.1304 - 13ms/epoch - 13ms/step\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 3.0108 - accuracy: 0.1304 - 12ms/epoch - 12ms/step\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 2.9883 - accuracy: 0.1304 - 10ms/epoch - 10ms/step\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 2.9656 - accuracy: 0.1739 - 13ms/epoch - 13ms/step\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 2.9427 - accuracy: 0.1739 - 14ms/epoch - 14ms/step\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 2.9198 - accuracy: 0.2174 - 14ms/epoch - 14ms/step\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 2.8969 - accuracy: 0.2174 - 11ms/epoch - 11ms/step\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 2.8743 - accuracy: 0.2609 - 11ms/epoch - 11ms/step\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 2.8512 - accuracy: 0.2609 - 14ms/epoch - 14ms/step\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 2.8279 - accuracy: 0.3043 - 11ms/epoch - 11ms/step\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 2.8042 - accuracy: 0.3043 - 11ms/epoch - 11ms/step\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 2.7801 - accuracy: 0.3043 - 12ms/epoch - 12ms/step\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 2.7557 - accuracy: 0.3478 - 12ms/epoch - 12ms/step\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 2.7309 - accuracy: 0.3478 - 11ms/epoch - 11ms/step\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 2.7061 - accuracy: 0.3478 - 13ms/epoch - 13ms/step\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 2.6813 - accuracy: 0.3913 - 12ms/epoch - 12ms/step\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 2.6564 - accuracy: 0.3913 - 11ms/epoch - 11ms/step\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 2.6315 - accuracy: 0.4348 - 11ms/epoch - 11ms/step\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 2.6063 - accuracy: 0.4783 - 13ms/epoch - 13ms/step\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 2.5808 - accuracy: 0.5217 - 11ms/epoch - 11ms/step\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 2.5553 - accuracy: 0.5217 - 15ms/epoch - 15ms/step\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 2.5298 - accuracy: 0.5217 - 20ms/epoch - 20ms/step\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 2.5044 - accuracy: 0.5217 - 12ms/epoch - 12ms/step\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 2.4789 - accuracy: 0.5652 - 12ms/epoch - 12ms/step\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 2.4533 - accuracy: 0.5652 - 13ms/epoch - 13ms/step\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 2.4275 - accuracy: 0.5652 - 12ms/epoch - 12ms/step\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 2.4014 - accuracy: 0.5652 - 10ms/epoch - 10ms/step\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 2.3749 - accuracy: 0.5652 - 11ms/epoch - 11ms/step\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 2.3481 - accuracy: 0.5652 - 10ms/epoch - 10ms/step\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 2.3212 - accuracy: 0.5652 - 11ms/epoch - 11ms/step\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 2.2940 - accuracy: 0.5652 - 10ms/epoch - 10ms/step\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 2.2667 - accuracy: 0.5652 - 11ms/epoch - 11ms/step\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 2.2411 - accuracy: 0.5652 - 10ms/epoch - 10ms/step\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 2.2235 - accuracy: 0.6087 - 10ms/epoch - 10ms/step\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 2.1869 - accuracy: 0.6087 - 10ms/epoch - 10ms/step\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 2.1878 - accuracy: 0.5652 - 11ms/epoch - 11ms/step\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 2.1447 - accuracy: 0.6087 - 10ms/epoch - 10ms/step\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 2.1436 - accuracy: 0.6957 - 11ms/epoch - 11ms/step\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 2.1161 - accuracy: 0.6957 - 10ms/epoch - 10ms/step\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 2.0735 - accuracy: 0.7391 - 19ms/epoch - 19ms/step\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 2.0586 - accuracy: 0.7826 - 10ms/epoch - 10ms/step\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 2.0394 - accuracy: 0.7826 - 10ms/epoch - 10ms/step\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 2.0026 - accuracy: 0.8261 - 11ms/epoch - 11ms/step\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 1.9900 - accuracy: 0.8261 - 11ms/epoch - 11ms/step\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 1.9707 - accuracy: 0.8261 - 10ms/epoch - 10ms/step\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 1.9394 - accuracy: 0.8261 - 14ms/epoch - 14ms/step\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 1.9116 - accuracy: 0.8261 - 10ms/epoch - 10ms/step\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 1.9028 - accuracy: 0.8261 - 10ms/epoch - 10ms/step\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 1.8664 - accuracy: 0.8261 - 10ms/epoch - 10ms/step\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 1.8501 - accuracy: 0.8696 - 10ms/epoch - 10ms/step\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 1.8318 - accuracy: 0.8696 - 9ms/epoch - 9ms/step\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 1.8035 - accuracy: 0.8696 - 10ms/epoch - 10ms/step\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 1.7805 - accuracy: 0.8696 - 10ms/epoch - 10ms/step\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 1.7646 - accuracy: 0.8696 - 11ms/epoch - 11ms/step\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 1.7352 - accuracy: 0.8696 - 9ms/epoch - 9ms/step\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 1.7188 - accuracy: 0.8696 - 9ms/epoch - 9ms/step\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 1.6967 - accuracy: 0.8696 - 10ms/epoch - 10ms/step\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 1.6701 - accuracy: 0.8696 - 15ms/epoch - 15ms/step\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 1.6542 - accuracy: 0.9130 - 10ms/epoch - 10ms/step\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 1.6277 - accuracy: 0.8696 - 11ms/epoch - 11ms/step\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 1.6080 - accuracy: 0.8696 - 10ms/epoch - 10ms/step\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 1.5880 - accuracy: 0.8696 - 12ms/epoch - 12ms/step\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 1.5634 - accuracy: 0.8696 - 10ms/epoch - 10ms/step\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 1.5461 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 1.5211 - accuracy: 0.9130 - 11ms/epoch - 11ms/step\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 1.5027 - accuracy: 0.8696 - 14ms/epoch - 14ms/step\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 1.4804 - accuracy: 0.9130 - 9ms/epoch - 9ms/step\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 1.4608 - accuracy: 0.9565 - 15ms/epoch - 15ms/step\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 1.4381 - accuracy: 0.9130 - 11ms/epoch - 11ms/step\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 1.4193 - accuracy: 0.9130 - 10ms/epoch - 10ms/step\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 1.3973 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 1.3796 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 1.3602 - accuracy: 0.9130 - 10ms/epoch - 10ms/step\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 1.3378 - accuracy: 0.9130 - 12ms/epoch - 12ms/step\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 1.3263 - accuracy: 0.9565 - 29ms/epoch - 29ms/step\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 1.3197 - accuracy: 0.9130 - 14ms/epoch - 14ms/step\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 1.2983 - accuracy: 0.9130 - 12ms/epoch - 12ms/step\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 1.2687 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 1.2439 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 1.2313 - accuracy: 0.9565 - 14ms/epoch - 14ms/step\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 1.2085 - accuracy: 0.9565 - 18ms/epoch - 18ms/step\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 1.1996 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 1.1845 - accuracy: 0.9130 - 10ms/epoch - 10ms/step\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 1.1694 - accuracy: 0.9130 - 11ms/epoch - 11ms/step\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 1.1401 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 1.1431 - accuracy: 0.9565 - 13ms/epoch - 13ms/step\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 1.1336 - accuracy: 0.9130 - 10ms/epoch - 10ms/step\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 1.1395 - accuracy: 0.9130 - 9ms/epoch - 9ms/step\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 1.1119 - accuracy: 0.9130 - 12ms/epoch - 12ms/step\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 1.0712 - accuracy: 0.9565 - 15ms/epoch - 15ms/step\n",
            "Epoch 102/200\n",
            "1/1 - 0s - loss: 1.0689 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 103/200\n",
            "1/1 - 0s - loss: 1.0435 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 1.0280 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 1.0156 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 1.0048 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 107/200\n",
            "1/1 - 0s - loss: 0.9867 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 108/200\n",
            "1/1 - 0s - loss: 0.9662 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 109/200\n",
            "1/1 - 0s - loss: 0.9595 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 110/200\n",
            "1/1 - 0s - loss: 0.9444 - accuracy: 0.9565 - 16ms/epoch - 16ms/step\n",
            "Epoch 111/200\n",
            "1/1 - 0s - loss: 0.9259 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 112/200\n",
            "1/1 - 0s - loss: 0.9151 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 113/200\n",
            "1/1 - 0s - loss: 0.9039 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 114/200\n",
            "1/1 - 0s - loss: 0.8886 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 115/200\n",
            "1/1 - 0s - loss: 0.8752 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 116/200\n",
            "1/1 - 0s - loss: 0.8622 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 117/200\n",
            "1/1 - 0s - loss: 0.8521 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 118/200\n",
            "1/1 - 0s - loss: 0.8372 - accuracy: 0.9565 - 13ms/epoch - 13ms/step\n",
            "Epoch 119/200\n",
            "1/1 - 0s - loss: 0.8259 - accuracy: 0.9565 - 14ms/epoch - 14ms/step\n",
            "Epoch 120/200\n",
            "1/1 - 0s - loss: 0.8165 - accuracy: 0.9565 - 15ms/epoch - 15ms/step\n",
            "Epoch 121/200\n",
            "1/1 - 0s - loss: 0.8022 - accuracy: 0.9565 - 15ms/epoch - 15ms/step\n",
            "Epoch 122/200\n",
            "1/1 - 0s - loss: 0.7911 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 123/200\n",
            "1/1 - 0s - loss: 0.7818 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 124/200\n",
            "1/1 - 0s - loss: 0.7682 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 125/200\n",
            "1/1 - 0s - loss: 0.7591 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 126/200\n",
            "1/1 - 0s - loss: 0.7481 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 127/200\n",
            "1/1 - 0s - loss: 0.7373 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 128/200\n",
            "1/1 - 0s - loss: 0.7273 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 129/200\n",
            "1/1 - 0s - loss: 0.7176 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 130/200\n",
            "1/1 - 0s - loss: 0.7064 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 131/200\n",
            "1/1 - 0s - loss: 0.6982 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 132/200\n",
            "1/1 - 0s - loss: 0.6875 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 133/200\n",
            "1/1 - 0s - loss: 0.6783 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 134/200\n",
            "1/1 - 0s - loss: 0.6694 - accuracy: 0.9565 - 19ms/epoch - 19ms/step\n",
            "Epoch 135/200\n",
            "1/1 - 0s - loss: 0.6600 - accuracy: 0.9565 - 13ms/epoch - 13ms/step\n",
            "Epoch 136/200\n",
            "1/1 - 0s - loss: 0.6513 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 137/200\n",
            "1/1 - 0s - loss: 0.6423 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 138/200\n",
            "1/1 - 0s - loss: 0.6334 - accuracy: 0.9565 - 21ms/epoch - 21ms/step\n",
            "Epoch 139/200\n",
            "1/1 - 0s - loss: 0.6256 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 140/200\n",
            "1/1 - 0s - loss: 0.6175 - accuracy: 0.9565 - 17ms/epoch - 17ms/step\n",
            "Epoch 141/200\n",
            "1/1 - 0s - loss: 0.6094 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 142/200\n",
            "1/1 - 0s - loss: 0.6008 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 143/200\n",
            "1/1 - 0s - loss: 0.5928 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 144/200\n",
            "1/1 - 0s - loss: 0.5848 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 145/200\n",
            "1/1 - 0s - loss: 0.5775 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 146/200\n",
            "1/1 - 0s - loss: 0.5697 - accuracy: 0.9565 - 13ms/epoch - 13ms/step\n",
            "Epoch 147/200\n",
            "1/1 - 0s - loss: 0.5625 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 148/200\n",
            "1/1 - 0s - loss: 0.5557 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 149/200\n",
            "1/1 - 0s - loss: 0.5502 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 150/200\n",
            "1/1 - 0s - loss: 0.5466 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 151/200\n",
            "1/1 - 0s - loss: 0.5424 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 152/200\n",
            "1/1 - 0s - loss: 0.5326 - accuracy: 0.9565 - 15ms/epoch - 15ms/step\n",
            "Epoch 153/200\n",
            "1/1 - 0s - loss: 0.5217 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 154/200\n",
            "1/1 - 0s - loss: 0.5191 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 155/200\n",
            "1/1 - 0s - loss: 0.5162 - accuracy: 0.9565 - 12ms/epoch - 12ms/step\n",
            "Epoch 156/200\n",
            "1/1 - 0s - loss: 0.5066 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 157/200\n",
            "1/1 - 0s - loss: 0.5043 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 158/200\n",
            "1/1 - 0s - loss: 0.4984 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 159/200\n",
            "1/1 - 0s - loss: 0.4926 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 160/200\n",
            "1/1 - 0s - loss: 0.4819 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 161/200\n",
            "1/1 - 0s - loss: 0.4779 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 162/200\n",
            "1/1 - 0s - loss: 0.4779 - accuracy: 0.9565 - 11ms/epoch - 11ms/step\n",
            "Epoch 163/200\n",
            "1/1 - 0s - loss: 0.4712 - accuracy: 0.9565 - 10ms/epoch - 10ms/step\n",
            "Epoch 164/200\n",
            "1/1 - 0s - loss: 0.4619 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 165/200\n",
            "1/1 - 0s - loss: 0.4614 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 166/200\n",
            "1/1 - 0s - loss: 0.4503 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 167/200\n",
            "1/1 - 0s - loss: 0.4478 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 168/200\n",
            "1/1 - 0s - loss: 0.4509 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 169/200\n",
            "1/1 - 0s - loss: 0.4394 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 170/200\n",
            "1/1 - 0s - loss: 0.4401 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 171/200\n",
            "1/1 - 0s - loss: 0.4361 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 172/200\n",
            "1/1 - 0s - loss: 0.4316 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 173/200\n",
            "1/1 - 0s - loss: 0.4235 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 174/200\n",
            "1/1 - 0s - loss: 0.4122 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 175/200\n",
            "1/1 - 0s - loss: 0.4106 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
            "Epoch 176/200\n",
            "1/1 - 0s - loss: 0.4040 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 177/200\n",
            "1/1 - 0s - loss: 0.4002 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 178/200\n",
            "1/1 - 0s - loss: 0.3957 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 179/200\n",
            "1/1 - 0s - loss: 0.3908 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 180/200\n",
            "1/1 - 0s - loss: 0.3877 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 181/200\n",
            "1/1 - 0s - loss: 0.3809 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 182/200\n",
            "1/1 - 0s - loss: 0.3766 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 183/200\n",
            "1/1 - 0s - loss: 0.3733 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
            "Epoch 184/200\n",
            "1/1 - 0s - loss: 0.3696 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 185/200\n",
            "1/1 - 0s - loss: 0.3649 - accuracy: 1.0000 - 9ms/epoch - 9ms/step\n",
            "Epoch 186/200\n",
            "1/1 - 0s - loss: 0.3604 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 187/200\n",
            "1/1 - 0s - loss: 0.3593 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 188/200\n",
            "1/1 - 0s - loss: 0.3541 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 189/200\n",
            "1/1 - 0s - loss: 0.3507 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n",
            "Epoch 190/200\n",
            "1/1 - 0s - loss: 0.3471 - accuracy: 1.0000 - 10ms/epoch - 10ms/step\n",
            "Epoch 191/200\n",
            "1/1 - 0s - loss: 0.3416 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 192/200\n",
            "1/1 - 0s - loss: 0.3388 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 193/200\n",
            "1/1 - 0s - loss: 0.3347 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
            "Epoch 194/200\n",
            "1/1 - 0s - loss: 0.3324 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 195/200\n",
            "1/1 - 0s - loss: 0.3287 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
            "Epoch 196/200\n",
            "1/1 - 0s - loss: 0.3250 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 197/200\n",
            "1/1 - 0s - loss: 0.3226 - accuracy: 1.0000 - 18ms/epoch - 18ms/step\n",
            "Epoch 198/200\n",
            "1/1 - 0s - loss: 0.3179 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
            "Epoch 199/200\n",
            "1/1 - 0s - loss: 0.3149 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
            "Epoch 200/200\n",
            "1/1 - 0s - loss: 0.3115 - accuracy: 1.0000 - 11ms/epoch - 11ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f779f17b410>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
        "    init_word = current_word\n",
        "    sentence = ''\n",
        "\n",
        "    # n번 반복\n",
        "    for _ in range(n):\n",
        "        # 현재 단어에 대한 정수 인코딩과 패딩\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items(): \n",
        "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        current_word = current_word + ' '  + word\n",
        "\n",
        "        # 예측 단어를 문장에 저장\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "aimXrzIHu5QV"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, '묵뚝뚝한', 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt9tJn5vvcyd",
        "outputId": "71293d25-f1ca-4e52-caf1-a6cf63f28490"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "묵뚝뚝한 야윈 야윈 데다가\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, '음울해', 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfBMyxBZvhu_",
        "outputId": "1af70189-10bb-4193-e6e3-cea331338c40"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "음울해 야윈 했다그럼에도\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, tokenizer, '결코', 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bmXtvB6v3wi",
        "outputId": "9c9cdd6b-1b8b-423c-ace9-44b86aa58827"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "결코 야윈 웃는 법이 없는 어터슨에게는\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NlshAysZwA4l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}