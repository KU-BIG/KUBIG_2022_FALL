{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6주차_과제_공도웅.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KU-BIG/KUBIG_2022_FALL/blob/main/1.%20%EB%B0%A9%ED%95%99%EB%B6%84%EB%B0%98/%EB%94%A5%EB%9F%AC%EB%8B%9D/%EA%B3%BC%EC%A0%9C/6%EC%A3%BC%EC%B0%A8/6%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1_%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6_%EA%B3%B5%EB%8F%84%EC%9B%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nmQ5F7UAeKB_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task1\n",
        "\n",
        "빈 칸을 채워주세요!\n",
        "\n",
        "단계별 output이 github 파일에는 남아있으니 그 output과 동일한 형태인지 확인하면서 진행해주시면 됩니다~"
      ],
      "metadata": {
        "id": "Sgxd6SxmeVcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 생성할 문장 데이터\n",
        "\n",
        "sentence = (\"Brick walls are there for a reason and you must not think \"\n",
        "            \"that the brick walls aren't there to keep us out, but rather \"\n",
        "            \"in this way that the brick walls are there to show us how badly we want things.\")"
      ],
      "metadata": {
        "id": "NDvUeC8BoUb6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. 문자 집합 만들기\n",
        "world_set = list(set(sentence))\n",
        "\n",
        "## 문제(1): 각 문자에 정수 인코딩 (공백도 하나의 원소로 포함)\n",
        "vocab = {value : i for i,value in enumerate(world_set)}"
      ],
      "metadata": {
        "id": "b9lkrKyZf8ie"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_0we5Y-gYDq",
        "outputId": "029e182d-cf0f-4c51-9cb9-fe08f3b4888d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'g': 0, 'e': 1, 'f': 2, 'm': 3, 'u': 4, 'B': 5, 'l': 6, 'w': 7, 'd': 8, 'p': 9, 'a': 10, 's': 11, '.': 12, ',': 13, 'h': 14, 'r': 15, 'o': 16, 'b': 17, 'k': 18, 'i': 19, 't': 20, ' ': 21, 'y': 22, 'n': 23, \"'\": 24, 'c': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. 문자 집합 크기 확인\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('문자 집합 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpKupU6lgpfT",
        "outputId": "6d2f7d8e-1527-4271-fbfc-a38a63341cc4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합 크기 : 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
        "\n",
        "hidden_size = vocab_size # 같아야 하는 것 확인!\n",
        "sequence_length = 10  # 너무 길거나 너무 짧게 잡으면 안됩니다!\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "wFDZJHSMg9In"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. seqence 길이 단위 자르기\n",
        "\n",
        "# 데이터 구성을 위한 리스트\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "## 문제(2): 반복문 내에서의 인덱싱을 사용하여 sequence_length 값 단위로 샘플을 잘라 데이터 만들기, y_str은 x_str은 한 칸씩 쉬프트된 sequnce\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "  x_str = sentence[i:i+sequence_length]\n",
        "  y_str = sentence[i+1:i+1+sequence_length]\n",
        "  print(i, x_str, \"->\", y_str)\n",
        "\n",
        "  # x_str과 y_str이 문자집합에 해당하는 인덱스를 각각 x_data, y_data에 append\n",
        "  x_data.append([vocab[c] for c in x_str])\n",
        "  y_data.append([vocab[d] for d in y_str])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbDcmJmghN7V",
        "outputId": "4bed5e56-e925-4b2f-d879-086e4c425a86"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Brick wall -> rick walls\n",
            "1 rick walls -> ick walls \n",
            "2 ick walls  -> ck walls a\n",
            "3 ck walls a -> k walls ar\n",
            "4 k walls ar ->  walls are\n",
            "5  walls are -> walls are \n",
            "6 walls are  -> alls are t\n",
            "7 alls are t -> lls are th\n",
            "8 lls are th -> ls are the\n",
            "9 ls are the -> s are ther\n",
            "10 s are ther ->  are there\n",
            "11  are there -> are there \n",
            "12 are there  -> re there f\n",
            "13 re there f -> e there fo\n",
            "14 e there fo ->  there for\n",
            "15  there for -> there for \n",
            "16 there for  -> here for a\n",
            "17 here for a -> ere for a \n",
            "18 ere for a  -> re for a r\n",
            "19 re for a r -> e for a re\n",
            "20 e for a re ->  for a rea\n",
            "21  for a rea -> for a reas\n",
            "22 for a reas -> or a reaso\n",
            "23 or a reaso -> r a reason\n",
            "24 r a reason ->  a reason \n",
            "25  a reason  -> a reason a\n",
            "26 a reason a ->  reason an\n",
            "27  reason an -> reason and\n",
            "28 reason and -> eason and \n",
            "29 eason and  -> ason and y\n",
            "30 ason and y -> son and yo\n",
            "31 son and yo -> on and you\n",
            "32 on and you -> n and you \n",
            "33 n and you  ->  and you m\n",
            "34  and you m -> and you mu\n",
            "35 and you mu -> nd you mus\n",
            "36 nd you mus -> d you must\n",
            "37 d you must ->  you must \n",
            "38  you must  -> you must n\n",
            "39 you must n -> ou must no\n",
            "40 ou must no -> u must not\n",
            "41 u must not ->  must not \n",
            "42  must not  -> must not t\n",
            "43 must not t -> ust not th\n",
            "44 ust not th -> st not thi\n",
            "45 st not thi -> t not thin\n",
            "46 t not thin ->  not think\n",
            "47  not think -> not think \n",
            "48 not think  -> ot think t\n",
            "49 ot think t -> t think th\n",
            "50 t think th ->  think tha\n",
            "51  think tha -> think that\n",
            "52 think that -> hink that \n",
            "53 hink that  -> ink that t\n",
            "54 ink that t -> nk that th\n",
            "55 nk that th -> k that the\n",
            "56 k that the ->  that the \n",
            "57  that the  -> that the b\n",
            "58 that the b -> hat the br\n",
            "59 hat the br -> at the bri\n",
            "60 at the bri -> t the bric\n",
            "61 t the bric ->  the brick\n",
            "62  the brick -> the brick \n",
            "63 the brick  -> he brick w\n",
            "64 he brick w -> e brick wa\n",
            "65 e brick wa ->  brick wal\n",
            "66  brick wal -> brick wall\n",
            "67 brick wall -> rick walls\n",
            "68 rick walls -> ick walls \n",
            "69 ick walls  -> ck walls a\n",
            "70 ck walls a -> k walls ar\n",
            "71 k walls ar ->  walls are\n",
            "72  walls are -> walls aren\n",
            "73 walls aren -> alls aren'\n",
            "74 alls aren' -> lls aren't\n",
            "75 lls aren't -> ls aren't \n",
            "76 ls aren't  -> s aren't t\n",
            "77 s aren't t ->  aren't th\n",
            "78  aren't th -> aren't the\n",
            "79 aren't the -> ren't ther\n",
            "80 ren't ther -> en't there\n",
            "81 en't there -> n't there \n",
            "82 n't there  -> 't there t\n",
            "83 't there t -> t there to\n",
            "84 t there to ->  there to \n",
            "85  there to  -> there to k\n",
            "86 there to k -> here to ke\n",
            "87 here to ke -> ere to kee\n",
            "88 ere to kee -> re to keep\n",
            "89 re to keep -> e to keep \n",
            "90 e to keep  ->  to keep u\n",
            "91  to keep u -> to keep us\n",
            "92 to keep us -> o keep us \n",
            "93 o keep us  ->  keep us o\n",
            "94  keep us o -> keep us ou\n",
            "95 keep us ou -> eep us out\n",
            "96 eep us out -> ep us out,\n",
            "97 ep us out, -> p us out, \n",
            "98 p us out,  ->  us out, b\n",
            "99  us out, b -> us out, bu\n",
            "100 us out, bu -> s out, but\n",
            "101 s out, but ->  out, but \n",
            "102  out, but  -> out, but r\n",
            "103 out, but r -> ut, but ra\n",
            "104 ut, but ra -> t, but rat\n",
            "105 t, but rat -> , but rath\n",
            "106 , but rath ->  but rathe\n",
            "107  but rathe -> but rather\n",
            "108 but rather -> ut rather \n",
            "109 ut rather  -> t rather i\n",
            "110 t rather i ->  rather in\n",
            "111  rather in -> rather in \n",
            "112 rather in  -> ather in t\n",
            "113 ather in t -> ther in th\n",
            "114 ther in th -> her in thi\n",
            "115 her in thi -> er in this\n",
            "116 er in this -> r in this \n",
            "117 r in this  ->  in this w\n",
            "118  in this w -> in this wa\n",
            "119 in this wa -> n this way\n",
            "120 n this way ->  this way \n",
            "121  this way  -> this way t\n",
            "122 this way t -> his way th\n",
            "123 his way th -> is way tha\n",
            "124 is way tha -> s way that\n",
            "125 s way that ->  way that \n",
            "126  way that  -> way that t\n",
            "127 way that t -> ay that th\n",
            "128 ay that th -> y that the\n",
            "129 y that the ->  that the \n",
            "130  that the  -> that the b\n",
            "131 that the b -> hat the br\n",
            "132 hat the br -> at the bri\n",
            "133 at the bri -> t the bric\n",
            "134 t the bric ->  the brick\n",
            "135  the brick -> the brick \n",
            "136 the brick  -> he brick w\n",
            "137 he brick w -> e brick wa\n",
            "138 e brick wa ->  brick wal\n",
            "139  brick wal -> brick wall\n",
            "140 brick wall -> rick walls\n",
            "141 rick walls -> ick walls \n",
            "142 ick walls  -> ck walls a\n",
            "143 ck walls a -> k walls ar\n",
            "144 k walls ar ->  walls are\n",
            "145  walls are -> walls are \n",
            "146 walls are  -> alls are t\n",
            "147 alls are t -> lls are th\n",
            "148 lls are th -> ls are the\n",
            "149 ls are the -> s are ther\n",
            "150 s are ther ->  are there\n",
            "151  are there -> are there \n",
            "152 are there  -> re there t\n",
            "153 re there t -> e there to\n",
            "154 e there to ->  there to \n",
            "155  there to  -> there to s\n",
            "156 there to s -> here to sh\n",
            "157 here to sh -> ere to sho\n",
            "158 ere to sho -> re to show\n",
            "159 re to show -> e to show \n",
            "160 e to show  ->  to show u\n",
            "161  to show u -> to show us\n",
            "162 to show us -> o show us \n",
            "163 o show us  ->  show us h\n",
            "164  show us h -> show us ho\n",
            "165 show us ho -> how us how\n",
            "166 how us how -> ow us how \n",
            "167 ow us how  -> w us how b\n",
            "168 w us how b ->  us how ba\n",
            "169  us how ba -> us how bad\n",
            "170 us how bad -> s how badl\n",
            "171 s how badl ->  how badly\n",
            "172  how badly -> how badly \n",
            "173 how badly  -> ow badly w\n",
            "174 ow badly w -> w badly we\n",
            "175 w badly we ->  badly we \n",
            "176  badly we  -> badly we w\n",
            "177 badly we w -> adly we wa\n",
            "178 adly we wa -> dly we wan\n",
            "179 dly we wan -> ly we want\n",
            "180 ly we want -> y we want \n",
            "181 y we want  ->  we want t\n",
            "182  we want t -> we want th\n",
            "183 we want th -> e want thi\n",
            "184 e want thi ->  want thin\n",
            "185  want thin -> want thing\n",
            "186 want thing -> ant things\n",
            "187 ant things -> nt things.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력해서 한 칸씩 쉬프트된 것 확인하기!\n",
        "\n",
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVFlILiOixdc",
        "outputId": "faf9cf59-2e31-4e14-ba03-4fee7cc69de8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 15, 19, 25, 18, 21, 7, 10, 6, 6]\n",
            "[15, 19, 25, 18, 21, 7, 10, 6, 6, 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##6. 입력 시퀀스에 대해 원핫인코딩 수행\n",
        "\n",
        "## 문제(4) : x_data를 원핫인코딩 > numpy의 eye를 쓸 수 있지 않을까?\n",
        "x_one_hot = [np.eye(len(vocab))[x] for x in x_data]\n",
        "\n",
        "##7. 입력 데이터, 레이블데이터 텐서로 변환\n",
        "\n",
        "## 문제(5) : x_one_hot과 y_data 텐서로 변환 : 둘 다 같은 형식의 텐서로 변환하면 될까?? (FloatTensor, LongTesor 중 맞는 것은?)\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "5lPes1dvjlNb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##8. 크기 확인\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMZzZlaymMk8",
        "outputId": "599a9b0f-50b0-44d2-e498-769564d900e5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([188, 10, 26])\n",
            "레이블의 크기 : torch.Size([188, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##9.원핫인코딩 결과 샘플 확인하기\n",
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knx1DE_AmSFB",
        "outputId": "8e968f52-e1bf-4a14-ce66-de61693f3663"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##10. 레이블 데이터 샘플 확인하기\n",
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pWDiH1SmYT_",
        "outputId": "63b09b61-7e3a-43fa-824d-53418fa5fd12"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([15, 19, 25, 18, 21,  7, 10,  6,  6, 11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV6s5TETZVFu",
        "outputId": "448253bc-1955-4598-e9a1-cb0cf8989756"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([188, 10, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##11. RNN 모델 구현\n",
        "\n",
        "##문제(6) : 기본 pytorch 인자 넣기 연습 + forward 채우기\n",
        "### 조건 : rnn layer 2개 쌓기 + 마지막은 fc layer\n",
        "### batch_fisrt 설정 필요할까? (유튜브 강의 참고)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.layers = layers\n",
        "\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim,layers,batch_first = True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim ,vocab_size)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.layers,X.size()[0],self.hidden_dim) #(2,188,26)\n",
        "\n",
        "    # Propagate input through RNN\n",
        "    # Input: (batch, seq_len, input_size) => X.size() = (188,10,26)\n",
        "    # hidden: (num_layers * num_directions, batch, hidden_size) =>rnn(X)[1].size() = (2,188,26)\n",
        "\n",
        "    out, _ = self.rnn(x,h0)\n",
        "    out = self.fc(out)\n",
        "  \n",
        "    return out"
      ],
      "metadata": {
        "id": "-Ww22xu8mfUc"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-0-QsGui9DD",
        "outputId": "3da507a0-c19d-45fc-c2d4-49192443db25"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([188, 10, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = torch.nn.RNN(26, 26,2,batch_first = True)"
      ],
      "metadata": {
        "id": "Rn_5C6VsjAvU"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn(X)[0].size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwswFpvdjHCn",
        "outputId": "0642d089-8cb6-4ed3-ba16-908a9a679a44"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([188, 10, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn(X)[1].size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RPytVJljSVs",
        "outputId": "38772549-0b02-483b-a830-61841662512c"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 188, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(2,X.size()[0],26).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTv-xL0jirhj",
        "outputId": "ed397c9e-d855-4a1a-f45e-455702bd2805"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 188, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(vocab_size, hidden_size, 2) #(26,26,2)"
      ],
      "metadata": {
        "id": "No2GRvTpnLBl"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##12. loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "##13. optimizer\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "9-zuJLeUnQLB"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##14. 출력 크기 점검\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-RxRaiHnh9U",
        "outputId": "8727e31a-5c7d-4db9-e19f-ad76e07cb6aa"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([188, 10, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.view(-1,26).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD-e_BkVcSYJ",
        "outputId": "9d59cafc-a710-40bd-c749-9a962248d432"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1880, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.view(-1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fCltT2wYMUy",
        "outputId": "00d08303-1362-49ee-a42f-ace640f5d27f"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1880])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.view(-1,26)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWnjriP6eo5g",
        "outputId": "545b8b1d-92bc-4817-fe80-ad47ea30b44d"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.7998, -1.0167, -2.1204,  ...,  0.3425, -4.2237, -2.6636],\n",
              "        [ 0.6412,  4.8120,  2.1728,  ..., -1.0013,  0.6091, -0.8956],\n",
              "        [ 1.0872,  2.5565, -3.6666,  ...,  4.1055, -1.0850,  8.7898],\n",
              "        ...,\n",
              "        [ 5.8168,  4.2398, -0.9342,  ..., -2.5350,  4.1850, -0.0251],\n",
              "        [ 0.0806,  2.6463, -3.5842,  ..., -0.6897,  0.0505, -0.7754],\n",
              "        [ 0.5870, -0.9869,  3.0740,  ...,  1.2036,  2.7333, -1.8418]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.view(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGepZLKGexG5",
        "outputId": "63d1758b-59a9-4629-f3d6-f5b3f4f96ca7"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([15, 19, 25,  ...,  0, 11, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##15. Training 시작\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    ##문제(7) : outputs, Y 형태 그대로 넣으면 안되죠. view 함수를 이용해 loss값을 계산해봅시다.\n",
        "    #output의 shape : 188,10,26\n",
        "    #label의 shape : 188,10\n",
        "    loss = criterion(outputs.view(-1,26), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #16. 예측결과 확인\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오기\n",
        "            predict_str += ''.join([world_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += world_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxxrxCd2nwoo",
        "outputId": "233db27f-273a-47bd-fd83-327d38b4df54"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "euiiieeieiiiiuiiiyiyiiiyiiiuiiiiiiuiieieiiiiiiiuiyiuiyiiieiiiiuiyiuiiiiieeieiiiiuiieiuiyiyiiiyuieiiiiiiiiiiuiiiiiiiyiyiiiiiyiiieieuiyiiuiyiuiiiiieeieiiiiuiiiyiyiiiyuiyiiiiiiiiiiiiiuiuieuiiiiiiyiiii\n",
            "ete   e          e    e     e       e  e   e    e    e     s    e   ee   e        e    e                  e     e  e e e   er  e    e    e   ee   e          e        e        f       e  e     e    \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                                                                                                                                                                     \n",
            "                                                   t          t         t                                      t                       t         t                                                   \n",
            "     t         t     t                             t     t    t         t            t     t                   t                  t e  t         t         t     t                                   \n",
            "     t         t e   t            t   t t          t e   t e  t e t e   t         t  t e   t t            t    t   e       e      t e  t e t e   t         t e   t t            t     t       t e    \n",
            "     ta    t   t e t t   t        t   t t t    t   t e   t e  t e t e   t     t   t  t e t t t     t      t    t   e t     e   a  t e  t e t e   t     t   t e t t t     t      t   e t       t e    \n",
            "     ta    t e the t t e t t    t t e t t ta   t   t e   t e  t e t e   ta    t e t  t e t t t t   ta     t    t   e e e   e  ta  t e  t e t e   ta    t e the t t t t   t      the e ta      the    \n",
            " e   tae e tae the e t e tht    t t e the tae tt   the   the  the the   tae e tae t  the t t e t   ta     t    th  e e e   e  ta  the  the the   tae e tae the e t e ta  th  a  the e ta  a   the    \n",
            " e   tae   tae the e the tht    t t e the tae  t   the   the  the the   tae   tae ta the t the t   ta  a  t ae th  e e a  he  tae the  the the   tae   tae the e the ta  th  a  the e ta  ae  the    \n",
            " e   tae   tae the e tae the    t the the  ae  th  the   the  the the   tae   tae ta the e the t   ta  a  taae th  e e a  he  tae the  the the   tae   tae the e the ta  th  a  the e ta  ae  the    \n",
            " e   tae   aae the e tae the    t the the  ae  th  the   the  the the   tae   aae tahthe e the a   ta ta  tahe th  e e a  he  tae the  the the   tae   aae the e the ta  th ta  the e ta  ae  the    \n",
            " e   tae   aae the e tae the    t the the  ae hthe the   the  the the   tae   aae ta the e the a   ua ta  tthe th  e e a  he  tae the  the the   tae   aae the e the ae  th ta  the e ta  ae  the    \n",
            " e   tall  aae the e toe taee   t tae the  ae htht the   the  the the   tall  aae ta the e the a   ua ta htthe th  e e a the  ta  the  the the   tall  aae the e the ae  ta ta  the e ta ta   the    \n",
            " e   th ls aae the e toe ta e   t ua  the  as htht the   the  the the   ta ls aae ta the e the h   ua aa htthe th  e e h ther ta  the  the the   ta ls aae the e the ae  ta aa  the e aa ta   the    \n",
            " e   th es are the e toe a  e     ua  tha aas htht ther  thee the the   ta es are ta the e the h   aa aa httat th  e e h ther  a  the  the the   ta es are the e the ae  ta aa  tha e aa ta   then   \n",
            " e   ta es are the e to  a  e     ua  tha aas htht ther  thes the the   ta es are ta the e the h  taa aa httat th  e e h thes  a  thel the the   ta es are the e tht he  ta aa  taa e aa ta   thes   \n",
            " er  ta es are the e to  a  e     ur  thataas htht thes  thes the the k ta es are ta the e tht h   ua aa httat ta  e e n thes  a  thee the the k ta es are the e tht he  ta aat taa e aa ta   thes   \n",
            " er  ta es are the e tos ar e t   ur  thataas htht thes  thes the the k ta es are ta the e the h   ua aa h tae ta  e e n thes  a  thee the the k ta es are the e tht ae  ta aat aaa e aa ta   thes   \n",
            " er  talls are the e tos ar e t s url tha aas htht thes  thes the the k talls are ta the e the h   ua aa h tae ta  e t n thes  a  thee the the k talls are the e the ae  ta aat ata e aa ta   thes   \n",
            " es  talls are the e tos ar e t s url tha aas htat thes  thes the taeck talls are ta the e the a   ua aa h tae ta  e t n thes  a  thee the taeck talls are the e the ae  ta uat aaa e aa ta   thes   \n",
            " esk talls are the e tos arre t s url tha uas  tat ther  thes the taeck talls are ta the e the a   ua aa h tus tas e ttn thes  a  thes the taeck talls are the e the ue  us uat uaa s aa tar  then   \n",
            " esk talls are the e tos urre t s url thu uas  tat therk thes the taeck talls are ta the e tht ur  us aa h tus tas e ttn thes  as thes the taeck talls are the e tht ua  us uat uai s aa tar  then   \n",
            " esk talls are the e to  urre t s url thu mus  tat therk thes the taeck talls are tt the e tht ur  us uu h tus tat e ttn then  as thet the taeck talls are the e tht ua  us uat wai s ua tar  thenk  \n",
            " eck talls are the e to  urre t s url thu mus  tat therk thes the taeck talls are tt the e th  hr  us au h tus tat e ets then  an thet the taeck talls are the e th  ua  us uat wai s aa ta l thenk  \n",
            " eck talls are the e to  urre s s url thu mus  tat therk thes the taeck talls are tt the e th  h   us au h tus tat e ets then aan thet the taeck talls are the e th  ua  us uat wails aa ta l thenk  \n",
            " eck talls are the e to  urre s s url thu mus  tat therk thet the toeck talls are tt the e th  hrn us au h tus  et e ets then aan thet the toeck talls are the e th  hat ws hat wadls aa tarl thenks \n",
            " eck talls are the e to  urre s s url t u mus  tat thenk thrt the toeck talls are tt the e th  hrn us au h tus  et etets then aay thet the toeck talls are the e th  hat ws hat wadls aa tarl thenks \n",
            " eck talls are the e tor urae s n url t u mus  tat thenk thet thertoeck walls are tt the e th  hrn us hu h tus  et etets then aay thet the toeck walls are the e th  hat us hat wadls aa tarl thenks \n",
            "reck walls are the e tor urae s n url t u mus  tat thenk thet thertoeck walls are tt the e th  hrn us ha h tus  et etets then way thet thertoeck walls are the e th  hat us hat wadls aa tarl thenks \n",
            "reck walls are the e tor urae s n url t u mus  tot thenk whet thertoick walls are tt the e th  hrn us aa h wus  et etetn then way that thertoick walls are the e th  hat us hat wadls aa tarl thenks \n",
            "reck walls are the e tor urre t n url t u mus  tot thenk whet thertoick walls are tt the e th  hrn us aa h wus  et e etn then way that thertoick walls are the e th  hat us hat wadls wa tall thenks \n",
            "reck walls are there tor urre ton unl y u mus  tot thenk thet thertoick walls are tt the e th  hrn us aa h wus  et eretn then way that thertoick walls are there th  hat us uat wadls wa tall thenks \n",
            "reck walls are there tor arre ton unl you must tot thenk that thertoick walls are tt the e th  hrn us aath wus  at eretn then way that thertoick walls are there to  hat us uat wadly wa tall thenks \n",
            "reck walls are there tor arre son unl you must tot thenk that thertoick walls are tt the e to  orn us aath bus aatheretn then way thet thertoick walls are there to  hat us uaw badly wa tall thenks \n",
            "reck walls are there tor arre son unk you must tot thenk that thertoick walls are tt the e to  orn us huth bus aatheretn then way thet thertoick walls are there to  haw us uuw badly wa tall thenks \n",
            "reck walls are there tor arre son and you must tot thenk thet thertoick walls are tt the e to  orn us huth bus tatheretn then way thet thertoick walls are there to  haw us uuw budly wa tall thenks \n",
            "reck walls are there tor arre son and you must tot thenk thet thertrick walls are tt there to  orn us huth bus tatheretn then way thet thertrick walls are there to  haw us ouw budly wa tall thenks \n",
            "reck walls are there tor arrerson and you must tot thenk that thertrick walls are tt there to  hrn us huth bus tatheretn then way thet thertrick walls are there to  haw us ouw budly wa tall thenks \n",
            "reck walls are there tor arrerson and you must tot thenk that thertrick walls are tt there to  ern us huth bus tat eretn then way thet the trick walls are there to  haw us ouw budly wa tall thenks \n",
            "rick walls are there tor arrerson and you must tot thenk thot the trick walls are tt there to  ern us hut, but tat eretn then way thet the trick walls are there to  haw us ouw budly wa tall thenks \n",
            "rick walls are there tor arrerson and you must aot thenk thot the trick walls are tt there th  ern us hut, but tat erktn then way thet the trick walls are there to  how us huw budly wa tall thenks \n",
            "rick walls are there tor arrerson and you must aot thenk that the trick walls are tt there to  ern us hut, but tather tn then way thet the trick walls are there to  how us how budly wa wall thenks \n",
            "rick walls are there tor arrerson and you must aot thenk that the trick walls are tt there to  ern us hut, but tather tn then way that the trick walls are there to  how us how budly wa wanl thenkst\n",
            "rick walls are there tor arrerson and you must aot thenk that the trick walls are tt there to  eep us hut, but rather tn then way that the trick walls are there to  how us how budly wa wanl thenkst\n",
            "rick walls are there tor arrerson and you must aot thenk that the trick walls are tt there to  eep us hut, but rather tn then way that the trick walls are there to  how us how budly we wanl thenks \n",
            "rick walls are there tor arrerson and you must aot thenk that the trick walls are tt there to  eep us hut, but rather tn then way that the trick walls are there to  how us how budly we wanl thenks \n",
            "rick walls are there tor a rerson and you must aot thenk that the trick walls are tt there to  eep us hut, but rather tn then way that the trick walls are there to  how us how badly we wanl thenks \n",
            "rick walls are there tor a rerson and you must not think that the brick walls are 't there to  eep us hut, but rather tn then way that the brick walls are there to  how us how badly we wanl thenks \n",
            "rick walls are there tor a rerson and you must not think that the brick walls are 't there to  eep us hut, but rather tn then way that the brick walls are there to  how us how badly we wanl thenks \n",
            "rick walls are there tor a rerson and you must not think that the brick walls are 't there to  eep us hut, but rather tn then way that the brick walls are there to  how us how badly we wanl thenks \n",
            "rick walls are there tor a rerson and you must not think that the brick walls are 't there to  eep us hut, but rather wn thes way that the brick walls are there to  how us oow badly we wanl thenks \n",
            "rick walls are there tor a rerson and you must not think that the brick walls are 't there to  eep us hut, but rather wn thes way that the brick walls are there to  how us oow badly we want thenks \n",
            "rick walls are there tor a rerson and you must not think that the brick walls are 't there to  eep us hut, but rather in thes way that the brick walls are there to  how us oow badly we want thenks \n",
            "rick walls are there tor a rerson and you must not think that the brick walls are 't there to keep us hut, but rather in thes way that the brick walls are there to khow us oow badly we want thenks \n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in thes way that the brick walls are there to khow us oow badly we want thenks \n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but ratherein thes way that the brick walls are there to khow us oow badly we want thenks \n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but ratherein this way that the brick walls are there to khow us oow badly we want thenkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but ratherein this way that the brick walls are there to khow us oow badly we want thenkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us oow badly we want thenkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us oow badly we want thenkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us oow badly we want thenkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us oow badly we want thenkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us oow badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinkst\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksy\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksy\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksy\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksy\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to keep us out, but rather in this way that the brick walls are there to khow us how badly we want thinksy\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksy\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksy\n",
            "rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "8qUkbiw2t0Il",
        "outputId": "91b064af-9e59-4863-dbaf-fb9dc3e4eede"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"rick walls are there tor a reason and you must not think that the brick walls are 't there to seep us out, but rather in this way that the brick walls are there to show us how badly we want thinksy\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ClGD_HkycfOT",
        "outputId": "307b9aac-f12c-49fb-f68d-9f7f58546a20"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Brick walls are there for a reason and you must not think that the brick walls aren't there to keep us out, but rather in this way that the brick walls are there to show us how badly we want things.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과가 어떤가요?? 마지막 에폭의 문장이 그럴싸한가요?"
      ],
      "metadata": {
        "id": "PkIzDTdyvTHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task2\n",
        "\n",
        "위 sentence는 제가 임의로 생성한 문장들입니다.\n",
        "\n",
        "마음에 드시는 문구 가져오셔서 문장이 어떻게 생성되는지 확인해보세요! \n",
        "\n",
        "영어가 아닌 한국어로 시도해보는 것도 좋겠죠? \n",
        "\n",
        "수정이 많이 필요(토큰화 등) 할 수 있으나 한번 시도해보시는 것 권장드립니다 :)\n",
        "\n",
        "위 베이스라인은 어디든 수정하셔도 좋고 조금 더 자연스러운 문장이 나올 수 있게 다양한 시도를 해보세요!\n",
        "\n",
        "조건 : 문장 3개 이상, 연결성이 있는 문장을 \" \" 으로 구분하여 ( )에 넣기"
      ],
      "metadata": {
        "id": "kN1zL8Dpvane"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "#%cd Mecab-ko-for-Google-Colab\n",
        "#!bash install_mecab-ko_on_colab190912.sh"
      ],
      "metadata": {
        "id": "X6I66OAXj6JQ"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 생성할 문장 데이터\n",
        "text = (\"우리 매번 같은 시간에 있다는 거 \"\n",
        "        \"시간을 보면 네가 내 안에 그려져 \"\n",
        "        \"같이 있어도 너만 생각할게\")"
      ],
      "metadata": {
        "id": "IKp-lKrjvXR9"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. 문자 집합 만들기\n",
        "from konlpy.tag import Mecab\n",
        "tokenizer = Mecab()\n",
        "word = tokenizer.morphs(text)\n",
        "word_set =list(set(word))\n",
        "\n",
        "\n",
        "## 문제(1): 각 문자에 정수 인코딩 (공백도 하나의 원소로 포함)\n",
        "vocab = {value : i for i,value in enumerate(word_set)}"
      ],
      "metadata": {
        "id": "rYE6PGNljqFE"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsY89qzqju0V",
        "outputId": "5a8aed20-e1c3-49d2-cfe7-e181c1d11e73"
      },
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'가': 0, '시간': 1, '생각': 2, '네': 3, '너': 4, '같': 5, '할': 6, '다는': 7, '은': 8, '을': 9, '보': 10, '어도': 11, '만': 12, '내': 13, '에': 14, '거': 15, '게': 16, '있': 17, '매번': 18, '면': 19, '우리': 20, '같이': 21, '그려져': 22, '안': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy8H2jdPoFrZ",
        "outputId": "d92262e2-38fb-4626-e3eb-8dae7bb9f2a9"
      },
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['우리', '매번', '같', '은', '시간', '에', '있', '다는', '거', '시간', '을', '보', '면', '네', '가', '내', '안', '에', '그려져', '같이', '있', '어도', '너', '만', '생각', '할', '게']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. 형태소 집합 크기 확인\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('형태소 집합 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N51mfbn3jzWH",
        "outputId": "0633f548-2054-4743-f93d-dd99603cc631"
      },
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 집합 크기 : 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 하이퍼 파라미터 설정(자유롭게 수정해보세요!)\n",
        "\n",
        "hidden_size = vocab_size # 같아야 하는 것 확인!\n",
        "sequence_length = 8  # 너무 길거나 너무 짧게 잡으면 안됩니다!\n",
        "learning_rate = 0.007"
      ],
      "metadata": {
        "id": "aJ-VDXAJks6E"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. seqence 길이 단위 자르기\n",
        "\n",
        "# 데이터 구성을 위한 리스트\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "## 문제(2): 반복문 내에서의 인덱싱을 사용하여 sequence_length 값 단위로 샘플을 잘라 데이터 만들기, y_str은 x_str은 한 칸씩 쉬프트된 sequnce\n",
        "\n",
        "for i in range(0, len(word) - sequence_length):\n",
        "  x_str = word[i:i+sequence_length]\n",
        "  y_str = word[i+1:i+1+sequence_length]\n",
        "  print(i, x_str, \"->\", y_str)\n",
        "\n",
        "  # x_str과 y_str이 문자집합에 해당하는 인덱스를 각각 x_data, y_data에 append\n",
        "  x_data.append([vocab[c] for c in x_str])\n",
        "  y_data.append([vocab[d] for d in y_str])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CblUUZzKlCBN",
        "outputId": "294f979d-9128-4202-c6d8-280ef1fbeae0"
      },
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ['우리', '매번', '같', '은', '시간', '에', '있', '다는'] -> ['매번', '같', '은', '시간', '에', '있', '다는', '거']\n",
            "1 ['매번', '같', '은', '시간', '에', '있', '다는', '거'] -> ['같', '은', '시간', '에', '있', '다는', '거', '시간']\n",
            "2 ['같', '은', '시간', '에', '있', '다는', '거', '시간'] -> ['은', '시간', '에', '있', '다는', '거', '시간', '을']\n",
            "3 ['은', '시간', '에', '있', '다는', '거', '시간', '을'] -> ['시간', '에', '있', '다는', '거', '시간', '을', '보']\n",
            "4 ['시간', '에', '있', '다는', '거', '시간', '을', '보'] -> ['에', '있', '다는', '거', '시간', '을', '보', '면']\n",
            "5 ['에', '있', '다는', '거', '시간', '을', '보', '면'] -> ['있', '다는', '거', '시간', '을', '보', '면', '네']\n",
            "6 ['있', '다는', '거', '시간', '을', '보', '면', '네'] -> ['다는', '거', '시간', '을', '보', '면', '네', '가']\n",
            "7 ['다는', '거', '시간', '을', '보', '면', '네', '가'] -> ['거', '시간', '을', '보', '면', '네', '가', '내']\n",
            "8 ['거', '시간', '을', '보', '면', '네', '가', '내'] -> ['시간', '을', '보', '면', '네', '가', '내', '안']\n",
            "9 ['시간', '을', '보', '면', '네', '가', '내', '안'] -> ['을', '보', '면', '네', '가', '내', '안', '에']\n",
            "10 ['을', '보', '면', '네', '가', '내', '안', '에'] -> ['보', '면', '네', '가', '내', '안', '에', '그려져']\n",
            "11 ['보', '면', '네', '가', '내', '안', '에', '그려져'] -> ['면', '네', '가', '내', '안', '에', '그려져', '같이']\n",
            "12 ['면', '네', '가', '내', '안', '에', '그려져', '같이'] -> ['네', '가', '내', '안', '에', '그려져', '같이', '있']\n",
            "13 ['네', '가', '내', '안', '에', '그려져', '같이', '있'] -> ['가', '내', '안', '에', '그려져', '같이', '있', '어도']\n",
            "14 ['가', '내', '안', '에', '그려져', '같이', '있', '어도'] -> ['내', '안', '에', '그려져', '같이', '있', '어도', '너']\n",
            "15 ['내', '안', '에', '그려져', '같이', '있', '어도', '너'] -> ['안', '에', '그려져', '같이', '있', '어도', '너', '만']\n",
            "16 ['안', '에', '그려져', '같이', '있', '어도', '너', '만'] -> ['에', '그려져', '같이', '있', '어도', '너', '만', '생각']\n",
            "17 ['에', '그려져', '같이', '있', '어도', '너', '만', '생각'] -> ['그려져', '같이', '있', '어도', '너', '만', '생각', '할']\n",
            "18 ['그려져', '같이', '있', '어도', '너', '만', '생각', '할'] -> ['같이', '있', '어도', '너', '만', '생각', '할', '게']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력해서 한 칸씩 쉬프트된 것 확인하기!\n",
        "\n",
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3G9H_TBoi7b",
        "outputId": "c719816b-1481-433e-fa79-690532e4cc8b"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 18, 5, 8, 1, 14, 17, 7]\n",
            "[18, 5, 8, 1, 14, 17, 7, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##6. 입력 시퀀스에 대해 원핫인코딩 수행\n",
        "\n",
        "## 문제(4) : x_data를 원핫인코딩 > numpy의 eye를 쓸 수 있지 않을까?\n",
        "x_one_hot = [np.eye(len(vocab))[x] for x in x_data]\n",
        "\n",
        "##7. 입력 데이터, 레이블데이터 텐서로 변환\n",
        "\n",
        "## 문제(5) : x_one_hot과 y_data 텐서로 변환 : 둘 다 같은 형식의 텐서로 변환하면 될까?? (FloatTensor, LongTesor 중 맞는 것은?)\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "cl6X9bVJonHT"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##8. 크기 확인\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpG9wGOjlGQ9",
        "outputId": "e39b43c3-6f15-4288-8038-e8d7d71db54a"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([19, 8, 24])\n",
            "레이블의 크기 : torch.Size([19, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##11. RNN 모델 구현\n",
        "\n",
        "##문제(6) : 기본 pytorch 인자 넣기 연습 + forward 채우기\n",
        "### 조건 : rnn layer 2개 쌓기 + 마지막은 fc layer\n",
        "### batch_fisrt 설정 필요할까? (유튜브 강의 참고)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.layers = layers\n",
        "\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim,layers,batch_first = True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim ,vocab_size)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.layers,X.size()[0],self.hidden_dim) #(2,26,17)\n",
        "\n",
        "    # Propagate input through RNN\n",
        "    # Input: (batch, seq_len, input_size) => X.size() = (26,3,17)\n",
        "    # hidden: (num_layers * num_directions, batch, hidden_size) =>rnn(X)[1].size() = (2,26,17)\n",
        "\n",
        "    out, _ = self.rnn(x,h0)\n",
        "    out = self.fc(out)\n",
        "  \n",
        "    return out"
      ],
      "metadata": {
        "id": "9oQDbNAjlJdw"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(vocab_size, hidden_size, 2) #(17,17,2)\n",
        "\n",
        "##12. loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "##13. optimizer\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
        "##14. 출력 크기 점검\n",
        "outputs = net(X)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T28mKEHllPyE",
        "outputId": "dc222efb-c869-46ac-c260-6b257760771d"
      },
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([19, 8, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TIe9y1so_b-",
        "outputId": "f6f98f51-575e-4712-e934-1650dd559bf6"
      },
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([19, 8, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 336
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-QY0ZYBpDqM",
        "outputId": "9d866d69-c26b-4074-fb99-b1eb32283ff4"
      },
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([19, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 337
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##15. Training 시작\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1,24), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #16. 예측결과 확인\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"우리\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오기\n",
        "            predict_str += ''.join([word_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += word_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS930czRlSbq",
        "outputId": "399686c3-97da-48ba-c877-3bf20870b4b4"
      },
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n",
            "우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6iy15LqBlUd7",
        "outputId": "ef50e02d-0faa-4780-b4cd-e8c00c51cc8b"
      },
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'우리매번같은시간에있다는거시간을보면네가내안에그려져같이있어도너만생각할게'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SuvY7F6opHMT",
        "outputId": "08d58238-bd2f-4d85-e79c-067caf5f1823"
      },
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'우리 매번 같은 시간에 있다는 거 시간을 보면 네가 내 안에 그려져 같이 있어도 너만 생각할게'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 348
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lxQUrVx3pQLK"
      },
      "execution_count": 340,
      "outputs": []
    }
  ]
}